<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chapter 17: Boosting on Introduction to Machine Learning (I2ML)</title><link>https://slds-lmu.github.io/i2ml/chapters/17_boosting/</link><description>Recent content in Chapter 17: Boosting on Introduction to Machine Learning (I2ML)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/i2ml/chapters/17_boosting/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 17.01: Introduction to Boosting / AdaBoost</title><link>https://slds-lmu.github.io/i2ml/chapters/17_boosting/17-01-intro-adaboost/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/17_boosting/17-01-intro-adaboost/</guid><description>&lt;p>In this section, we introduce the pioneering AdaBoost algorithm.&lt;/p></description></item><item><title>Chapter 17.02: Boosting Concept</title><link>https://slds-lmu.github.io/i2ml/chapters/17_boosting/17-02-gradient-boosting-concept/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/17_boosting/17-02-gradient-boosting-concept/</guid><description>&lt;p>In this section, we discuss the general boosting principle: performing gradient descent in function space by repeatedly fitting new base learner components to the current pseudo-residuals.&lt;/p></description></item><item><title>Chapter 17.03: Boosting Illustration</title><link>https://slds-lmu.github.io/i2ml/chapters/17_boosting/17-03-gradient-boosting-illustration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/17_boosting/17-03-gradient-boosting-illustration/</guid><description>&lt;p>We show several illustrative regression examples to visualize the boosting
principle.&lt;/p></description></item><item><title>Chapter 17.04: Boosting Regularization</title><link>https://slds-lmu.github.io/i2ml/chapters/17_boosting/17-04-gradient-boosting-regularization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/17_boosting/17-04-gradient-boosting-regularization/</guid><description>&lt;p>Powerful boosting learners tend to overfit. We discuss the number of iterations, base learner complexity, and shrinkage as countermeasures.&lt;/p></description></item><item><title>Chapter 17.05: Boosting for Classification</title><link>https://slds-lmu.github.io/i2ml/chapters/17_boosting/17-05-gradient-boosting-classification/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/17_boosting/17-05-gradient-boosting-classification/</guid><description>&lt;p>We introduce boosting algorithms for both binary and multiclass classification with several examples.&lt;/p></description></item><item><title>Chapter 17.06: Gradient Boosting with Trees I</title><link>https://slds-lmu.github.io/i2ml/chapters/17_boosting/17-06-gradient-boosting-trees-1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/17_boosting/17-06-gradient-boosting-trees-1/</guid><description>&lt;p>We discuss trees as the most popular base learners in gradient boosting, with special emphasis on model structure and interaction depth.&lt;/p></description></item><item><title>Chapter 17.07: Gradient Boosting with Trees II</title><link>https://slds-lmu.github.io/i2ml/chapters/17_boosting/17-07-gradient-boosting-trees-2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/17_boosting/17-07-gradient-boosting-trees-2/</guid><description>&lt;p>We explain how terminal coefficients are found in a risk-minimal manner and briefly discuss tree-based boosting for multiclass problems.&lt;/p></description></item><item><title>Chapter 17.08: XGBoost</title><link>https://slds-lmu.github.io/i2ml/chapters/17_boosting/17-08-gradient-boosting-xgboost/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/17_boosting/17-08-gradient-boosting-xgboost/</guid><description>&lt;p>We introduce XGBoost, a highly efficient, tree-based boosting system with additional regularizers.&lt;/p></description></item><item><title>Chapter 17.09: Component Wise Boosting Basics 1</title><link>https://slds-lmu.github.io/i2ml/chapters/17_boosting/17-09-cwb-1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/17_boosting/17-09-cwb-1/</guid><description>&lt;p>We introduce the concept of CWB, common base learners and built-in feature selection.&lt;/p></description></item><item><title>Chapter 17.10: Component Wise Boosting Basics 2</title><link>https://slds-lmu.github.io/i2ml/chapters/17_boosting/17-10-cwb-2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/17_boosting/17-10-cwb-2/</guid><description>&lt;p>We explain the handling of categorical features and of the intercept and introduce a practical example.&lt;/p></description></item><item><title>Chapter 17.11: CWB and GLMs</title><link>https://slds-lmu.github.io/i2ml/chapters/17_boosting/17-11-cwb-glm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/17_boosting/17-11-cwb-glm/</guid><description>&lt;p>We explain the relationship between CWB and GLMs.&lt;/p></description></item><item><title>Chapter 17.12: Advanced CWB</title><link>https://slds-lmu.github.io/i2ml/chapters/17_boosting/17-12-adv-cwb/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/17_boosting/17-12-adv-cwb/</guid><description>&lt;p>We explain the details of nonlinear BLs and splines, decomposition for splines, fair base learner selection and feature importance and PDPs.&lt;/p></description></item></channel></rss>