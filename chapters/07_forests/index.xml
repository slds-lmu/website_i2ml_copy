<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chapter 07: Random Forests on Introduction to Machine Learning (I2ML)</title><link>https://slds-lmu.github.io/i2ml/chapters/07_forests/</link><description>Recent content in Chapter 07: Random Forests on Introduction to Machine Learning (I2ML)</description><generator>Hugo</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/i2ml/chapters/07_forests/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 07.00: Random Forests: In a Nutshell</title><link>https://slds-lmu.github.io/i2ml/chapters/07_forests/07-00-nutshell-random-forest/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/07_forests/07-00-nutshell-random-forest/</guid><description>&lt;p>In this nutshell chunk, we delve into Random Forests, an ensemble method that harnesses multiple decision trees for improved prediction accuracy and robustness.&lt;/p></description></item><item><title>Chapter 07.01: Bagging Ensembles</title><link>https://slds-lmu.github.io/i2ml/chapters/07_forests/07-01-bagging/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/07_forests/07-01-bagging/</guid><description>&lt;p>Bagging (bootstrap aggregation) is a method for combining many models into a meta-model which often works much better than its individual components. In this section, we present the basic idea of bagging and explain why and when bagging works.&lt;/p></description></item><item><title>Chapter 07.02: Basics</title><link>https://slds-lmu.github.io/i2ml/chapters/07_forests/07-02-basics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/07_forests/07-02-basics/</guid><description>&lt;p>In this section we investigate random forests, a modification of bagging for trees.&lt;/p></description></item><item><title>Chapter 07.03: Out-of-Bag Error Estimate</title><link>https://slds-lmu.github.io/i2ml/chapters/07_forests/07-03-oob-error/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/07_forests/07-03-oob-error/</guid><description>&lt;p>We introduce the concepts of in-bag and out-of-bag observations and explain how to compute the out-of-bag error estimate.&lt;/p></description></item><item><title>Chapter 07.04: Feature Importance</title><link>https://slds-lmu.github.io/i2ml/chapters/07_forests/07-04-featureimportance/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/07_forests/07-04-featureimportance/</guid><description>&lt;p>In a complex machine learning model, the contributions of the different features to the model performance are difficult to evaluate. The concept of feature importance allows to quantify these effects for random forests.&lt;/p></description></item><item><title>Chapter 07.05: Proximities</title><link>https://slds-lmu.github.io/i2ml/chapters/07_forests/07-05-proximities/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/07_forests/07-05-proximities/</guid><description>&lt;p>The term &lt;em>proximity&lt;/em> refers to the &amp;ldquo;closeness&amp;rdquo; between pairs of cases. Proximities are calculated for each pair of observations and can be derived directly from random forests.&lt;/p></description></item></channel></rss>