<!DOCTYPE html>
<html><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="/i2ml/css/style.css">


<title>Introduction to Machine Learning (I2ML) | Chapter 18: Information Theory</title>


<link rel="apple-touch-icon" sizes="180x180" href="/i2ml/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/i2ml/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/i2ml/favicon-16x16.png">
<link rel="manifest" href="/i2ml/site.webmanifest">
<link rel="mask-icon" href="/i2ml/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">

</head><body>
<img id="logo" src="/i2ml/i2ml.svg" />

<div id="nav-border" class="container">
    <nav id="nav" class="nav justify-content-center">
        
        <a class="nav-link" href="/i2ml">
        
        Home
        </a>
        
        <a class="nav-link" href="/i2ml/chapters/">
        
        Chapters
        </a>
        
        <a class="nav-link" href="/i2ml/exercises/">
        
        Exercises
        </a>
        
        <a class="nav-link" href="/i2ml/appendix/">
        
        Appendix
        </a>
        
        <a class="nav-link" href="/i2ml/prerequisites/">
        
        Prerequisites
        </a>
        
        <a class="nav-link" href="/i2ml/literature/">
        
        Literature
        </a>
        
        <a class="nav-link" href="/i2ml/team/">
        
        Team
        </a>
        
        <a class="nav-link" href="/i2ml/news/">
        
        News
        </a>
        
    </nav>
</div><div id="content" class="container">
<h1>Chapter 18: Information Theory</h1>

<p><p>This chapter covers basic information-theoretic concepts and discusses their relation to machine learning.</p>
</p>


<div class="chapter_overview">
<ul class="list-unstyled">


<li>
    <a class="title" href="/i2ml/chapters/18_information_theory/18-01-entropy/">Chapter 18.01: Entropy</a>
    
      
        <p>We introduce entropy, which expresses the expected information for discrete random variables, as a central concept in information theory.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/18_information_theory/18-02-diffent/">Chapter 18.02: Differential Entropy</a>
    
      
        <p>In this section, we extend the definition of entropy to the continuous case.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/18_information_theory/18-03-kl/">Chapter 18.03: Kullback-Leibler Divergence</a>
    
      
        <p>The Kullback-Leibler divergence (KL) is an important quantity for measuring the difference between two probability distributions. We discuss different intuitions for KL and relate it to risk minimization and likelihood ratios.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/18_information_theory/18-04-sourcecoding/">Chapter 18.04: Entropy and Optimal Code Length</a>
    
      
        <p>In this section, we introduce source coding and discuss how entropy can be understood as optimal code length.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/18_information_theory/18-05-cross-entropy-kld/">Chapter 18.05: Cross-Entropy, KL and Source Coding</a>
    
      
        <p>We introduce cross-entropy as a further information-theoretic concept and discuss the connection between entropy, cross-entropy, and Kullback-Leibler divergence.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/18_information_theory/18-06-ml/">Chapter 18.06: Information Theory for Machine Learning</a>
    
      
        <p>In this section, we discuss how information-theoretic concepts are used in machine learning and demonstrate the equivalence of KL minimization and maximum likelihood maximization, as well as how (cross-)entropy can be used as a loss function.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/18_information_theory/18-07-mutual-info/">Chapter 18.07: Joint Entropy and Mutual Information</a>
    
      
        <p>Information theory also provides means of quantifying relations between two random variables that extend the concept of (linear) correlation. We discuss joint entropy, conditional entropy, and mutual information in this context.
</p>
      
      
</li>


</ul>
</div>


        </div><footer class="bg-light text-center text-lg-start fixed-bottom">
<ul class="list-inline text-center">
  <li class="list-inline-item">Â© 2022 Course Creator</li>
  
  <li class="list-inline-item"><a class="nav-link" href="/i2ml" target="_blank">Main Course Website</a></li>
  
  <li class="list-inline-item"><a class="nav-link" href="https://github.com/slds-lmu/lecture_i2ml" target="_blank">Material Source Code</a></li>
  
  <li class="list-inline-item"><a class="nav-link" href="https://github.com/slds-lmu/i2ml" target="_blank">Website Source Code</a></li>
  
</ul>
</footer>

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      ignoreHtmlClass: ['quizdown']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</body>
</html>
