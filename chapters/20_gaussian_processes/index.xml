<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chapter 20: Gaussian Processes on Introduction to Machine Learning (I2ML)</title><link>https://slds-lmu.github.io/i2ml/chapters/20_gaussian_processes/</link><description>Recent content in Chapter 20: Gaussian Processes on Introduction to Machine Learning (I2ML)</description><generator>Hugo</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/i2ml/chapters/20_gaussian_processes/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 20.01: The Bayesian Linear Model</title><link>https://slds-lmu.github.io/i2ml/chapters/20_gaussian_processes/20-01-bayes-lm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/20_gaussian_processes/20-01-bayes-lm/</guid><description>&lt;p>We begin by reviewing the Bayesian formulation of a linear model and show that instead of point estimates for parameters and predictions, we obtain an entire posterior and predictive distribution.&lt;/p></description></item><item><title>Chapter 20.02: Gaussian Processes</title><link>https://slds-lmu.github.io/i2ml/chapters/20_gaussian_processes/20-02-basic/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/20_gaussian_processes/20-02-basic/</guid><description>&lt;p>In this section, we introduce the basic idea behind Gaussian processes. We move from weight to function space and build some intuition on distributions over functions, discuss GPs&amp;rsquo; marginalization property, derive GP priors, and interpret GPs as indexed families.&lt;/p></description></item><item><title>Chapter 20.03: Covariance Functions for GPs</title><link>https://slds-lmu.github.io/i2ml/chapters/20_gaussian_processes/20-03-covariance/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/20_gaussian_processes/20-03-covariance/</guid><description>&lt;p>In this section, we discuss the role of covariance functions in GPs and introduce the most common choices.&lt;/p></description></item><item><title>Chapter 20.04: Gaussian Process Prediction</title><link>https://slds-lmu.github.io/i2ml/chapters/20_gaussian_processes/20-04-prediction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/20_gaussian_processes/20-04-prediction/</guid><description>&lt;p>In this section, we show how to derive the posterior process and discuss further properties of GPs as well as noisy GPs.&lt;/p></description></item><item><title>Chapter 20.05: Gaussian Process Training</title><link>https://slds-lmu.github.io/i2ml/chapters/20_gaussian_processes/20-05-training/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/20_gaussian_processes/20-05-training/</guid><description>&lt;p>In this section, we show how Gaussian processes are actually trained using maximum likelihood estimation and exploiting the fact that we can learn covariance functions&amp;rsquo; hyperparameters on the fly.&lt;/p></description></item></channel></rss>