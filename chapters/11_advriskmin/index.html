<!DOCTYPE html>
<html><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="/i2ml/css/style.css">


<title>Introduction to Machine Learning (I2ML) | Chapter 11: Advanced Risk Minimization</title>


<link rel="apple-touch-icon" sizes="180x180" href="/i2ml/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/i2ml/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/i2ml/favicon-16x16.png">
<link rel="manifest" href="/i2ml/site.webmanifest">
<link rel="mask-icon" href="/i2ml/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">

</head><body>
<img id="logo" src="/i2ml/i2ml.svg" />

<div id="nav-border" class="container">
    <nav id="nav" class="nav justify-content-center">
        
        <a class="nav-link" href="/i2ml">
        
        Home
        </a>
        
        <a class="nav-link" href="/i2ml/chapters/">
        
        Chapters
        </a>
        
        <a class="nav-link" href="/i2ml/exercises/">
        
        Exercises
        </a>
        
        <a class="nav-link" href="/i2ml/appendix/">
        
        Appendix
        </a>
        
        <a class="nav-link" href="/i2ml/prerequisites/">
        
        Prerequisites
        </a>
        
        <a class="nav-link" href="/i2ml/literature/">
        
        Literature
        </a>
        
        <a class="nav-link" href="/i2ml/team/">
        
        Team
        </a>
        
        <a class="nav-link" href="/i2ml/news/">
        
        News
        </a>
        
    </nav>
</div><div id="content" class="container">
<h1>Chapter 11: Advanced Risk Minimization</h1>

<p><p>This chapter revisits the theory of risk minimization, providing more in-depth analysis on established losses and the connection between empirical risk minimization and maximum likelihood estimation. We also introduce some more advanced loss functions for regression and classification.</p>
</p>


<div class="chapter_overview">
<ul class="list-unstyled">


<li>
    <a class="title" href="/i2ml/chapters/11_advriskmin/11-01-risk-minimizer/">Chapter 11.01: Risk Minimizers</a>
    
      
        <p>We introduce important concepts in theoretical risk minimization: risk minimizer, Bayes risk, Bayes regret, consistent learners and the optimal constant model.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/11_advriskmin/11-02-pseudo-residuals/">Chapter 11.02: Pseudo-Residuals</a>
    
      
        <p>We introduce the concept of pseudo-residuals, i.e., loss residuals in function space, and discuss their relation to gradient descent.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/11_advriskmin/11-03-regression-l2/">Chapter 11.03: L2 Loss</a>
    
      
        <p>In this section, we revisit \(L2\) loss and derive its risk minimizer &amp;ndash; the conditional mean &amp;ndash; and optimal constant model &amp;ndash; the empirical mean of observed target values.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/11_advriskmin/11-04-regression-l1/">Chapter 11.04: L1 Loss</a>
    
      
        <p>In this section, we revisit \(L1\) loss and derive its risk minimizer &amp;ndash; the conditional median &amp;ndash; and optimal constant model &amp;ndash; the empirical median of observed target values.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/11_advriskmin/11-05-regression-further-losses/">Chapter 11.05: Advanced Regression Losses</a>
    
      
        <p>In this section, we introduce and discuss the following advanced regression losses: Huber, log-cosh, Cauchy, log-barrier, epsilon-insensitive, and quantile loss.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/11_advriskmin/11-06-classification-01/">Chapter 11.06: 0-1 Loss</a>
    
      
        <p>In this section, we revisit the 0-1 loss and derive its risk minimizer .
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/11_advriskmin/11-07-classification-bernoulli/">Chapter 11.07: Bernoulli Loss</a>
    
      
        <p>We study the Bernoulli loss and derive its risk minimizer and optimal constant model. We further discuss the connection between Bernoulli loss minimization and tree splitting according to the entropy criterion.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/11_advriskmin/11-08-classification-logreg-deep-dive/">Chapter 11.08: Logistic Regression: Deep Dive</a>
    
      
        <p>In this segment, we derive the gradient and Hessian of logistice regression and show that logistic regression is a convex problem. This section is presented as a deep-dive. Please note that there are no videos accompanying this section.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/11_advriskmin/11-09-classification-brier/">Chapter 11.09: Brier Score</a>
    
      
        <p>In this section, we introduce the Brier score and derive its risk minimizer and optimal constant model. We further discuss the connection between Brier score minimization and tree splitting according to the Gini index.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/11_advriskmin/11-10-classification-further-losses/">Chapter 11.10: Advanced Classification Losses</a>
    
      
        <p>In this section, we introduce and discuss the following advanced classification losses: (squared) hinge loss, \(L2\) loss on scores, exponential loss, and AUC loss.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/11_advriskmin/11-11-classification-deep-dive/">Chapter 11.11: Optimal constant model for the empirical log loss risk</a>
    
      
        <p>In this segment, we explore the derivation of the optimal constant model concerning the empirical log loss risk. This section is presented as a deep-dive. Please note that there are no videos accompanying this section.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/11_advriskmin/11-12-max-likelihood-l2/">Chapter 11.12: Maximum Likelihood Estimation vs Empirical Risk Minimization I</a>
    
      
        <p>We discuss the connection between maximum likelihood estimation and risk minimization, then demonstrate the correspondence between a Gaussian error distribution and \(L2\) loss.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/11_advriskmin/11-13-max-likelihood-other/">Chapter 11.13: Maximum Likelihood Estimation vs Empirical Risk Minimization II</a>
    
      
        <p>We discuss the connection between maximum likelihood estimation and risk minimization for further losses (\(L1\) loss, Bernoulli loss).
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/11_advriskmin/11-14-losses-properties/">Chapter 11.14: Properties of Loss Functions</a>
    
      
        <p>We discuss the concept of robustness, analytical and functional properties of loss functions and how they may influence the convergence of optimizers.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/11_advriskmin/11-15-bias-variance-decomposition/">Chapter 11.15: Bias Variance Decomposition</a>
    
      
        <p>We discuss how to decompose the generalization error of a learner.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/11_advriskmin/11-16-bias-variance-deep-dive/">Chapter 11.16: Bias Variance Decomposition: Deep Dive</a>
    
      
        <p>In this segment, we discuss details of the decomposition of the generalization error of a learner. This section is presented as a deep-dive. Please note that there are no videos accompanying this section.
</p>
      
      
</li>


</ul>
</div>


        </div><footer class="bg-light text-center text-lg-start fixed-bottom">
<ul class="list-inline text-center">
  <li class="list-inline-item">Â© 2022 Course Creator</li>
  
  <li class="list-inline-item"><a class="nav-link" href="/i2ml" target="_blank">Main Course Website</a></li>
  
  <li class="list-inline-item"><a class="nav-link" href="https://github.com/slds-lmu/lecture_i2ml" target="_blank">Material Source Code</a></li>
  
  <li class="list-inline-item"><a class="nav-link" href="https://github.com/slds-lmu/i2ml" target="_blank">Website Source Code</a></li>
  
</ul>
</footer>

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      ignoreHtmlClass: ['quizdown']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</body>
</html>
