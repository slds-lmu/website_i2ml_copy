<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chapter 2: Supervised Regression on Introduction to Machine Learning (I2ML)</title><link>https://slds-lmu.github.io/i2ml/chapters/02_supervised_regression/</link><description>Recent content in Chapter 2: Supervised Regression on Introduction to Machine Learning (I2ML)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/i2ml/chapters/02_supervised_regression/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 02.01: Linear Models with L2 Loss</title><link>https://slds-lmu.github.io/i2ml/chapters/02_supervised_regression/02-01-l2-loss/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/02_supervised_regression/02-01-l2-loss/</guid><description>&lt;p>In this section, we focus on the general concept of linear regression and explain how the linear regression model can be used from a machine learning perspective to predict a continuous numerical target variable. Furthermore, we introduce the \(L2\) loss in the context of linear regression and explain how its use results in an SSE-minimal model.&lt;/p></description></item><item><title>Chapter 02.02: Deep Dive: Proof OLS Regression</title><link>https://slds-lmu.github.io/i2ml/chapters/02_supervised_regression/02-02-ols/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/02_supervised_regression/02-02-ols/</guid><description>&lt;p>In this section, we provide you with a proof for the ordinary least squares (OLS) method.&lt;/p></description></item><item><title>Chapter 02.03: Linear Models with L1 Loss</title><link>https://slds-lmu.github.io/i2ml/chapters/02_supervised_regression/02-03-l1-loss/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/02_supervised_regression/02-03-l1-loss/</guid><description>&lt;p>In this section, we introduce \(L1\) loss and elaborate its differences to \(L2\) loss. In addition, we explain how the choice of loss affects optimization and robustness.&lt;/p></description></item><item><title>Chapter 02.04: Polynomial Regression Models</title><link>https://slds-lmu.github.io/i2ml/chapters/02_supervised_regression/02-04-polynomials/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/02_supervised_regression/02-04-polynomials/</guid><description>&lt;p>This section introduces polynomials to obtain more flexible models for the regression task. We explain the connection to the basic linear model and discuss the problem of overfitting.&lt;/p></description></item></channel></rss>