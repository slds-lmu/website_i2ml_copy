<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chapter 21: Imbalanced Learning on Introduction to Machine Learning (I2ML)</title><link>https://slds-lmu.github.io/i2ml/chapters/21_imbalanced_learning/</link><description>Recent content in Chapter 21: Imbalanced Learning on Introduction to Machine Learning (I2ML)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/i2ml/chapters/21_imbalanced_learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 21.01: Introduction</title><link>https://slds-lmu.github.io/i2ml/chapters/21_imbalanced_learning/21-01-intro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/21_imbalanced_learning/21-01-intro/</guid><description>&lt;p>We define the phenomenon of imbalanced data sets and explain its consequences on accuarcy. Furthermore, we introduce some techniques for handling imbalanced data sets.&lt;/p></description></item><item><title>Chapter 21.02: Performance Measures</title><link>https://slds-lmu.github.io/i2ml/chapters/21_imbalanced_learning/21-02-perf-msr/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/21_imbalanced_learning/21-02-perf-msr/</guid><description>&lt;p>We introduce performance measures other than accuracy and explain their advantages over accuracy for imbalanced date. In addition we introduce extensions of these measures for multiclass settings.&lt;/p></description></item><item><title>Chapter 21.03: Cost-Sensitive Learning 1</title><link>https://slds-lmu.github.io/i2ml/chapters/21_imbalanced_learning/21-03-cs-1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/21_imbalanced_learning/21-03-cs-1/</guid><description>&lt;p>We introduce the concept of a Cost Matrix, the Minimum expected cost priciple and the optimal theoretical threshold.&lt;/p></description></item><item><title>Chapter 21.04: Cost-Sensitive Learning 2</title><link>https://slds-lmu.github.io/i2ml/chapters/21_imbalanced_learning/21-04-cs-2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/21_imbalanced_learning/21-04-cs-2/</guid><description>&lt;p>In this section we focus on empirical thresholding and model-agnostic Meta Costs.&lt;/p></description></item><item><title>Chapter 21.05: Cost-Sensitive Learning 3</title><link>https://slds-lmu.github.io/i2ml/chapters/21_imbalanced_learning/21-05-cs-3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/21_imbalanced_learning/21-05-cs-3/</guid><description>&lt;p>We explain the concepts of instance specific costs and cost-sensitive OVO.&lt;/p></description></item><item><title>Chapter 21.06: Cost Curves 1</title><link>https://slds-lmu.github.io/i2ml/chapters/21_imbalanced_learning/21-06-cc-1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/21_imbalanced_learning/21-06-cc-1/</guid><description>&lt;p>We introduce cost curves for misclassif error and explain the duality between ROC points and cost lines.&lt;/p></description></item><item><title>Chapter 21.07: Cost Curves 2</title><link>https://slds-lmu.github.io/i2ml/chapters/21_imbalanced_learning/21-07-cc-2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/21_imbalanced_learning/21-07-cc-2/</guid><description>&lt;p>We explain cost curves with cost matrices and comparing classifiers. In addition we do a wrap-up comparision to ROC.&lt;/p></description></item><item><title>Chapter 21.08: Sampling Methods 1</title><link>https://slds-lmu.github.io/i2ml/chapters/21_imbalanced_learning/21-08-smpl-1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/21_imbalanced_learning/21-08-smpl-1/</guid><description>&lt;p>We introduce the idea of sampling methods for dealing with imbalanced data. In addition, we explain certain undersampling techniques.&lt;/p></description></item><item><title>Chapter 21.09: Sampling Methods 2</title><link>https://slds-lmu.github.io/i2ml/chapters/21_imbalanced_learning/21-09-smpl-2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/21_imbalanced_learning/21-09-smpl-2/</guid><description>&lt;p>We introduce the state-of-art oversampling technique SMOTE.&lt;/p></description></item></channel></rss>