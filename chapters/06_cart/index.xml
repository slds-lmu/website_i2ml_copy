<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chapter 06: Classification and Regression Trees (CART) on Introduction to Machine Learning (I2ML)</title><link>https://slds-lmu.github.io/i2ml/chapters/06_cart/</link><description>Recent content in Chapter 06: Classification and Regression Trees (CART) on Introduction to Machine Learning (I2ML)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/i2ml/chapters/06_cart/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 06.00: CART: In a Nutshell</title><link>https://slds-lmu.github.io/i2ml/chapters/06_cart/06-00-nutshell-cart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/06_cart/06-00-nutshell-cart/</guid><description>&lt;p>In this nutshell chunk, we unravel the workings of CARTs (Classification and Regression Trees).&lt;/p></description></item><item><title>Chapter 06.01: Predictions with CART</title><link>https://slds-lmu.github.io/i2ml/chapters/06_cart/06-01-predictions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/06_cart/06-01-predictions/</guid><description>&lt;p>Decision trees are an important type of machine learning model and come in two main types: classification and regression trees. In this section, we explain the general idea of CART and show how they recursively divide up the input space into ever smaller rectangular partitions.
Thus, we think of CART for now only as a predictor.&lt;/p></description></item><item><title>Chapter 06.02: Growing a Tree</title><link>https://slds-lmu.github.io/i2ml/chapters/06_cart/06-02-treegrowing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/06_cart/06-02-treegrowing/</guid><description>&lt;p>In this section, we explain how to grow a tree starting with an empty tree, i.e., a root node containing all the data. It will be shown that trees are grown by recursively applying greedy optimization to each node.&lt;/p></description></item><item><title>Chapter 06.03: Splitting Criteria for Regression</title><link>https://slds-lmu.github.io/i2ml/chapters/06_cart/06-03-splitcriteria-regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/06_cart/06-03-splitcriteria-regression/</guid><description>&lt;p>CART algorithms require splitting criteria for trees, which are usually defined in terms of impurity reduction. In this section we formalize the idea of splitting criteria and explain the details of splitting. We start with regression and doing so we show how split criteria fit into our framework of empirical risk minimization.&lt;/p></description></item><item><title>Chapter 06.04: Splitting Criteria for Classification</title><link>https://slds-lmu.github.io/i2ml/chapters/06_cart/06-04-splitcriteria-classification/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/06_cart/06-04-splitcriteria-classification/</guid><description>&lt;p>We extend splitting criteria to classification task. Here, we see that there are analogies of ERM and impurity reduction. While these analogies are interested, proving the equivalence of ERM and impurity reduction are beyond the scope of this lecture. The interested reader can refer to chapter 11 of this lecture, where we proof the equivalence.&lt;/p></description></item><item><title>Chapter 06.05: Computational Aspects of Finding Splits</title><link>https://slds-lmu.github.io/i2ml/chapters/06_cart/06-05-computationalaspects/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/06_cart/06-05-computationalaspects/</guid><description>&lt;p>In this section, we explain the computational aspects of the node-splitting procedure, especially for nominal features. In addition, we illustrate how to deal with missing values.&lt;/p></description></item><item><title>Chapter 06.06: Stopping Criteria &amp; Pruning</title><link>https://slds-lmu.github.io/i2ml/chapters/06_cart/06-06-stoppingpruning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/06_cart/06-06-stoppingpruning/</guid><description>&lt;p>The recursive partitioning procedure used to grow a CART usually leads to problems such as exponential growth of computations, overfitting, and the horizon effect. To deal with these problems, we can use stopping criteria and pruning. In this section, we explain the basis of these two solutions.&lt;/p></description></item><item><title>Chapter 06.07: Discussion</title><link>https://slds-lmu.github.io/i2ml/chapters/06_cart/06-07-discussion/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/06_cart/06-07-discussion/</guid><description>&lt;p>In this section we discuss the advantages and disadvantages of CART and mention other tree methodologies.&lt;/p></description></item></channel></rss>