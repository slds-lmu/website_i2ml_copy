<!DOCTYPE html>
<html><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="/i2ml/css/style.css">


<title>Introduction to Machine Learning (I2ML) | Chapters</title>


<link rel="apple-touch-icon" sizes="180x180" href="/i2ml/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/i2ml/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/i2ml/favicon-16x16.png">
<link rel="manifest" href="/i2ml/site.webmanifest">
<link rel="mask-icon" href="/i2ml/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">

</head><body>
<img id="logo" src="/i2ml/i2ml.svg" />

<div id="nav-border" class="container">
    <nav id="nav" class="nav justify-content-center">
        
        <a class="nav-link" href="/i2ml">
        
        Home
        </a>
        
        <a class="nav-link" href="/i2ml/chapters/">
        
        Chapters
        </a>
        
        <a class="nav-link" href="/i2ml/exercises/">
        
        Exercises
        </a>
        
        <a class="nav-link" href="/i2ml/appendix/">
        
        Appendix
        </a>
        
        <a class="nav-link" href="/i2ml/prerequisites/">
        
        Prerequisites
        </a>
        
        <a class="nav-link" href="/i2ml/literature/">
        
        Literature
        </a>
        
        <a class="nav-link" href="/i2ml/team/">
        
        Team
        </a>
        
        <a class="nav-link" href="/i2ml/news/">
        
        News
        </a>
        
    </nav>
</div><div id="content" class="container">
<h1>Chapters</h1>

<p></p>


<div class="chapter_overview">
<ul class="list-unstyled">


<li>
    <a class="title" href="/i2ml/chapters/00_all/">All Slides Chapters 1-10 and 11-19</a>
    
      
        <p></p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/01_ml_basics/">Chapter 1: ML Basics</a>
    
      
        <p>This chapter introduces the basic concepts of Machine Learning. We focus on supervised learning, explain the difference between regression and classification, show how to evaluate and compare Machine Learning models and formalize the concept of learning.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/02_supervised_regression/">Chapter 2: Supervised Regression</a>
    
      
        <p>This chapter treats the supervised regression task in more detail. We will see different loss functions for regression, how a linear regression model can be used from a Machine Learning perspective, and how to extend it with polynomials for greater flexibility.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/03_supervised_classification/">Chapter 03: Supervised Classification</a>
    
      
        <p>This chapter treats the supervised classification task in more detail. We will see examples of binary and multiclass classification and the differences between discriminative and generative approaches. In particular, we will address logistic regression, discriminant analysis and naive Bayes classifiers.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/04_evaluation/">Chapter 04: Performance Evaluation</a>
    
      
        <p>This chapter treats the challenge of evaluating the performance of a model. We will introduce different performance measures for regression and classification tasks, explain the problem of overfitting as well as the difference between training and test error, and, lastly, present a variety of resampling techniques.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/05_knn/">Chapter 05: k-Nearest Neighbors (k-NN)</a>
    
      
        <p>This chapter addresses \(k\)-nearest neighbors, a distance-based algorithm suited to both regression and classification. Predictions are made based upon neighboring observations, assuming feature similarity translates to target similarity.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/06_cart/">Chapter 06: Classification and Regression Trees (CART)</a>
    
      
        <p>This chapter introduces Classification and Regression Trees (CART), a well-established machine learning procedure. We explain the main idea and give details on splitting criteria, discuss computational aspects of growing a tree, and illustrate the idea of stopping criteria and pruning.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/07_forests/">Chapter 07: Random Forests</a>
    
      
        <p>This chapter introduces bagging as a method to increase the performance of trees (or other base learners). A modification of bagging leads to random forests. We explain the main idea of random forests, benchmark their performance with the methods seen so far and show how to quantify the impact of a single feature on the performance of the random forest as well as how to compute proximities between observations.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/08_neural_networks/">Chapter 08: Neural Networks</a>
    
      
        <p>This chapter introduces the basic concepts of neural networks. We integrated chapters from our course on Deep Learning in order to be able to use (simple) neural networks for supervised ML on tabular data.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/09_tuning/">Chapter 09: Tuning</a>
    
      
        <p>This chapter introduces and formalizes the problem of hyperparameter tuning. We cover basic techniques such as grid search and random search as well as more advanced techniques like evolutionary algorithms, model-based optimization and multi-fidelity optimization.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/10_nested_resampling/">Chapter 10: Nested Resampling</a>
    
      
        <p>This chapter first defines the untouched-test-set principle and proceeds to explain the concepts of train-validation-test split and nested resampling.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/11_advriskmin/">Chapter 11: Advanced Risk Minimization</a>
    
      
        <p>This chapter revisits the theory of risk minimization, providing more in-depth analysis on established losses and the connection between empirical risk minimization and maximum likelihood estimation. We also introduce some more advanced loss functions for regression and classification.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/12_multiclass/">Chapter 12: Multiclass Classification</a>
    
      
        <p>This chapter treats the multiclass case of classification. Tasks with more than two classes preclude the application of some techniques studied in the binary scenario and require an adaptation of loss functions.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/13_information_theory/">Chapter 13: Information Theory</a>
    
      
        <p>This chapter covers basic information-theoretic concepts and discusses their relation to machine learning.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/14_cod/">Chapter 14: Curse of Dimensionality</a>
    
      
        <p>Frequently, our intuition developed in low-dimensional spaces does not generalize to higher dimensions. This chapter introduces the phenomenon of the curse of dimensionality and discusses its effects on the behavior of machine learning models.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/15_regularization/">Chapter 15: Regularization</a>
    
      
        <p>Regularization is a vital tool in machine learning to prevent overfitting and foster generalization ability. This chapter introduces the concept of regularization and discusses common regularization techniques in more depth.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/16_linear_svm/">Chapter 16: Linear Support Vector Machines</a>
    
      
        <p>This chapter introduces the linear support vector machine (SVM), a linear classifier that finds decision boundaries by maximizing margins to the closest data points, possibly allowing for violations to a certain extent.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/17_nonlinear_svm/">Chapter 17: Nonlinear Support Vector Machines</a>
    
      
        <p>Many classification problems warrant nonlinear decision boundaries. This chapter introduces nonlinear support vector machines as a crucial extension to the linear variant.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/18_boosting/">Chapter 18: Boosting</a>
    
      
        <p>This chapter introduces boosting as a sequential ensemble method that creates powerful committees from different kinds of base learners.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/19_feature_selection/">Chapter 19: Feature Selection</a>
    
      
        <p>This chapter introduces feature selection, i.e., dinding a well-performing, hopefully small set of features for a task.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/20_gaussian_processes/">Chapter 20: Gaussian Processes</a>
    
      
        <p>This chapter introduces Gaussian processes as a model class. Gaussian processes are non-parametric approaches with ubiquitous application that model entire distributions in function space.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/21_imbalanced_learning/">Chapter 21: Imbalanced Learning</a>
    
      
        <p>This chapter introduces techniques for learning on imbalanced datasets.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/22_multitarget_learning/">Chapter 22: Multitarget Learning</a>
    
      
        <p>This chapter introduces multitarget learning techniques.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/23_online_learning/">Chapter 23: Online Learning</a>
    
      
        <p>This chapter introduces online learning.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/coding_ml_python/">Coding ML [Python and sklearn]</a>
    
      
        <p>This section introduces basic concepts and implementations using Python and in particular sklearn.
</p>
      
      
</li>

<li>
    <a class="title" href="/i2ml/chapters/coding_ml_r/">Coding ML [R and mlr3]</a>
    
      
        <p>For an introduction to the R package mlr3 we recommend walking through some chapters of the mlr3 book as summarized in this document. After some basic concepts, this focuses on resampling, tuning and pipelines.
</p>
      
      
</li>


</ul>
</div>


        </div><footer class="bg-light text-center text-lg-start fixed-bottom">
<ul class="list-inline text-center">
  <li class="list-inline-item">© 2022 Course Creator</li>
  
  <li class="list-inline-item"><a class="nav-link" href="/i2ml" target="_blank">Main Course Website</a></li>
  
  <li class="list-inline-item"><a class="nav-link" href="https://github.com/slds-lmu/lecture_i2ml" target="_blank">Material Source Code</a></li>
  
  <li class="list-inline-item"><a class="nav-link" href="https://github.com/slds-lmu/i2ml" target="_blank">Website Source Code</a></li>
  
</ul>
</footer>

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      ignoreHtmlClass: ['quizdown']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</body>
</html>
