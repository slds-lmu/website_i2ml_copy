<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chapter 15: Regularization on Introduction to Machine Learning (I2ML)</title><link>https://slds-lmu.github.io/i2ml/chapters/15_regularization/</link><description>Recent content in Chapter 15: Regularization on Introduction to Machine Learning (I2ML)</description><generator>Hugo</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/i2ml/chapters/15_regularization/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 15.01: Introduction to Regularization</title><link>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-01-regu-intro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-01-regu-intro/</guid><description>&lt;p>In this section, we revisit overfitting and introduce regularization as a remedy.&lt;/p></description></item><item><title>Chapter 15.02: Ridge Regression</title><link>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-02-l2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-02-l2/</guid><description>&lt;p>We introduce Ridge regression as a key approach to regularizing linear models.&lt;/p></description></item><item><title>Chapter 15.03: Lasso Regression</title><link>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-03-l1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-03-l1/</guid><description>&lt;p>We introduce Lasso regression as a key approach to regularizing linear models.&lt;/p></description></item><item><title>Chapter 15.04: Lasso vs Ridge Regression</title><link>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-04-l1vsl12/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-04-l1vsl12/</guid><description>&lt;p>This section provides a detailed comparison between Lasso and Ridge regression.&lt;/p></description></item><item><title>Chapter 15.05: Elastic Net and Regularization for GLMs</title><link>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-05-enetlogreg/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-05-enetlogreg/</guid><description>&lt;p>In this section, we introduce the elastic net as a combination of Ridge and Lasso regression and discuss regularization for logistic regression.&lt;/p></description></item><item><title>Chapter 15.06: Other Types of Regularization</title><link>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-06-other/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-06-other/</guid><description>&lt;p>In this section, we introduce other regularization approaches besides the important special cases \(L1\) and \(L2\).&lt;/p></description></item><item><title>Chapter 15.07: Regularization in NonLinear Models</title><link>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-07-nonlin/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-07-nonlin/</guid><description>&lt;p>In this section, we demonstrate regularization in non-linear models like neural networks.&lt;/p></description></item><item><title>Chapter 15.08: Regularization and Bayesian Priors</title><link>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-08-bayes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-08-bayes/</guid><description>&lt;p>In this section, we motivate regularization from a Bayesian perspective, showing how different penalty terms correspond to different Bayesian priors.&lt;/p></description></item><item><title>Chapter 15.09: Geometric Analysis of L2 Regularization and Weight Decay</title><link>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-09-geom-l2-wdecay/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-09-geom-l2-wdecay/</guid><description>&lt;p>In this section, we provide a geometric understanding of \(L2\) regularization, showing how parameters are shrunk according to the eigenvalues of the Hessian of empirical risk, and discuss its correspondence to weight decay.&lt;/p></description></item><item><title>Chapter 15.10: Geometric Analysis of L1 Regularization</title><link>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-10-geom-l1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-10-geom-l1/</guid><description>&lt;p>In this section, we provide a geometric understanding of \(L1\) regularization and show that it encourages sparsity in the parameter vector.&lt;/p></description></item><item><title>Chapter 15.11: Early Stopping</title><link>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-11-early-stopping/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-11-early-stopping/</guid><description>&lt;p>In this section, we introduce early stopping and show how it can act as a regularizer.&lt;/p></description></item><item><title>Chapter 15.12: Details on Ridge Regression: Deep Dive</title><link>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-12-ridge-deep/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-12-ridge-deep/</guid><description>&lt;p>In this section, we consider Ridge regression as row-augmentation and as minimizing risk under feature noise. We also discuss the bias-variance tradeoff.&lt;/p></description></item><item><title>Chapter 15.13: Soft-thresholding and L1 regularization: Deep Dive</title><link>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-13-lasso-deep/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-13-lasso-deep/</guid><description>&lt;p>In this section, we prove the previously stated proposition regarding soft-thresholding and L1 regularization.&lt;/p></description></item></channel></rss>