<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chapter 15: Regularization on Introduction to Machine Learning (I2ML)</title><link>https://slds-lmu.github.io/i2ml/chapters/15_regularization/</link><description>Recent content in Chapter 15: Regularization on Introduction to Machine Learning (I2ML)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/i2ml/chapters/15_regularization/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 15.01: Introduction to Regularization</title><link>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-01-regu-intro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-01-regu-intro/</guid><description>&lt;p>In this section, we revisit overfitting and introduce regularization as a remedy.&lt;/p></description></item><item><title>Chapter 15.02: Lasso and Ridge Regression</title><link>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-02-l1l2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-02-l1l2/</guid><description>&lt;p>We introduce Lasso and Ridge regression as the key approaches to regularizing linear models.&lt;/p></description></item><item><title>Chapter 15.03: Lasso vs Ridge Regression</title><link>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-03-l1vsl12/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-03-l1vsl12/</guid><description>&lt;p>This section provides a detailed comparison between Lasso and Ridge regression.&lt;/p></description></item><item><title>Chapter 15.04: Elastic Net and Regularization for GLMs</title><link>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-04-enetlogreg/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-04-enetlogreg/</guid><description>&lt;p>In this section, we introduce the elastic net as a combination of Ridge and Lasso regression and discuss regularization for logistic regression.&lt;/p></description></item><item><title>Chapter 15.05: L0 Regularization</title><link>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-05-l0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-05-l0/</guid><description>&lt;p>In this section, we introduce \(LQ\) regularization and particularly discuss \(L0\) regularization as an important special case besides \(L1\) and \(L2\) that penalizes the number of non-zero parameters.&lt;/p></description></item><item><title>Chapter 15.06: Regularization in NonLinear Models and Bayesian Priors</title><link>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-06-nonlin-bayes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-06-nonlin-bayes/</guid><description>&lt;p>In this section, we motivate regularization from a Bayesian perspective, showing how different penalty terms correspond to different Bayesian priors.&lt;/p></description></item><item><title>Chapter 15.07: Geometric Analysis of L2 Regularization and Weight Decay</title><link>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-07-geom-l2-wdecay/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-07-geom-l2-wdecay/</guid><description>&lt;p>In this section, we provide a geometric understanding of \(L2\) regularization, showing how parameters are shrunk according to the eigenvalues of the Hessian of empirical risk, and discuss its correspondence to weight decay.&lt;/p></description></item><item><title>Chapter 15.08: Geometric Analysis of L1 Regularization</title><link>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-08-geom-l1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-08-geom-l1/</guid><description>&lt;p>In this section, we provide a geometric understanding of \(L1\) regularization and show that it encourages sparsity in the parameter vector.&lt;/p></description></item><item><title>Chapter 15.09: Soft-thresholding and L1 regularization: Deep Dive</title><link>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-09-lasso-deep/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-09-lasso-deep/</guid><description>&lt;p>In this section, we prove the previously stated proposition regarding soft-thresholding and L1 regularization.&lt;/p></description></item><item><title>Chapter 15.10: Early Stopping</title><link>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-10-early-stopping/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-10-early-stopping/</guid><description>&lt;p>In this section, we introduce early stopping and show how it can act as a regularizer.&lt;/p></description></item></channel></rss>