---
title: "Chapter 02.03: Linear Models with L1 Loss"
weight: 2003
quizdown: true
---
In this section, we introduce \\(L1\\) loss and elaborate its differences to \\(L2\\) loss. In addition, we explain how the choice of loss affects optimization and robustness.

<!--more-->

### Lecture video

{{< video id="kAZclFFAkhA" >}}

### Lecture slides

{{< pdfjs file="https://github.com/slds-lmu/lecture_i2ml/blob/master/slides-pdf/slides-regression-linearmodel-l1.pdf" >}}

### Code demo 

**Linear model & gradient descent**

You can run the code snippets in the demos on your local machine. The corresponding Rmd version of this demo can be found [here](https://github.com/compstat-lmu/lecture_i2ml/blob/master/code-demos/code_demo_limo.Rmd). If you want to render the Rmd files to PDF, you need the accompanying [style files](https://github.com/compstat-lmu/lecture_i2ml/tree/master/style). 

{{< pdfjs file="https://github.com/slds-lmu/lecture_i2ml/tree/master/code-demos-pdf/code_demo_limo.pdf" >}}

### Quiz

{{< quizdown >}}

---
shuffle_questions: false
---

## Which statements are true? 

- [ ] The absolute loss function is more sensitive to outliers than the quadratic loss function.
- [x] Optimization of $L2$ loss is easier than of $L1$ loss.

{{< /quizdown >}}


