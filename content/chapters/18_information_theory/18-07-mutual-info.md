---
title: "Chapter 18.07: Joint Entropy and Mutual Information"
weight: 18007
---
Information theory also provides means of quantifying relations between two random variables that extend the concept of (linear) correlation. We discuss joint entropy, conditional entropy, and mutual information in this context. 

<!--more-->

### Lecture video

{{< video id="ZoWANYz5KHE" >}}

### Lecture slides

{{< pdfjs file="https://github.com/slds-lmu/lecture_sl/raw/main/slides-pdf/slides-info-mutual-info.pdf" >}}
