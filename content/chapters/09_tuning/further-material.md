---
title: "Tuning: Further Material"
weight: 9006
---

<!--more-->
<!---
#### Literature cited in this chapter

{{< pdfjs file="https://github.com/slds-lmu/i2ml/blob/hpo-link/content/appendix/references.pdf" >}}
-->
#### Further Material

- Bischl, Bernd, et al. ["Hyperparameter optimization: Foundations, algorithms, best practices, and open challenges."](https://wires.onlinelibrary.wiley.com/doi/full/10.1002/widm.1484) Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery (2021): e1484.  
    This paper goes beyond grid search and random search and reviews important automatic hyperparameter optimization (HPO) methods, provides practical recommendations for conducting HPO, and discusses HPO algorithms, performance evaluation, combination with machine learning pipelines, runtime improvements, and parallelization.  
- mlr3 Practical Tuning Series:  
    These notebooks are a step-by-step hands-on tutorial on how to tune ML models with mlr3.
    - [Part I - Tune a Support Vector Machine](https://mlr-org.com/gallery/series/2021-03-09-practical-tuning-series-tune-a-support-vector-machine/) 
    - [Part II - Tune a Preprocessing Pipeline](https://mlr-org.com/gallery/series/2021-03-10-practical-tuning-series-tune-a-preprocessing-pipeline/)
    - [Part III - Build an Automated Machine Learning System](https://mlr-org.com/gallery/series/2021-03-11-practical-tuning-series-build-an-automated-machine-learning-system/)
    - [Part IV - Tuning and Parallel Processing](https://mlr-org.com/gallery/series/2021-03-12-practical-tuning-series-tuning-and-parallel-processing/)
