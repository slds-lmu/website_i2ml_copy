<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Introduction to Machine Learning (I2ML)</title><link>https://slds-lmu.github.io/website_i2ml_copy/</link><description>Recent content on Introduction to Machine Learning (I2ML)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/website_i2ml_copy/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 01.01: What is ML?</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/01_ml_basics/01-01-what_is_ml/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/01_ml_basics/01-01-what_is_ml/</guid><description>&lt;p>As subtopic of artificial intelligence, machine learning is a mathematically well-defined discipline and usually constructs predictive or decision models from data, instead of explicitly programming them. In this section, you will see some typical examples of where machine learning is applied and the main directions of machine learning.&lt;/p></description></item><item><title>Chapter 01.02: Data</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/01_ml_basics/01-02-data/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/01_ml_basics/01-02-data/</guid><description>&lt;p>In this section we explain the basic structure of tabular data used in machine learning. We will differentiate targets from features, talk about labeled and unlabeled data and introduce the concept of the data generating process.&lt;/p></description></item><item><title>Chapter 01.03: Tasks</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/01_ml_basics/01-03-tasks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/01_ml_basics/01-03-tasks/</guid><description>&lt;p>The tasks of supervised learning can roughly be divided in two categories: regression (for continuous outcome) and classification (for categorical outcome). We will present some examples.&lt;/p></description></item><item><title>Chapter 01.04: Models and Parameters</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/01_ml_basics/01-04-models-parameters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/01_ml_basics/01-04-models-parameters/</guid><description>&lt;p>We introduce models as functional hypotheses about the mapping from feature to target space that allow us to make predictions by computing a function of the input data. Frequently in machine learning, models are understood to be parameterized curves, which is illustrated by several examples.&lt;/p></description></item><item><title>Chapter 01.05: Learner</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/01_ml_basics/01-05-learner/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/01_ml_basics/01-05-learner/</guid><description>&lt;p>Roughly speaking, learners (endowed with a specific hyperparameter configuration) take training data and return a model.&lt;/p></description></item><item><title>Chapter 01.06: Losses and Risk Minimization</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/01_ml_basics/01-06-riskminimization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/01_ml_basics/01-06-riskminimization/</guid><description>&lt;p>In order to find good solutions we need a concept to evaluate and compare models. To this end, the concepts of &lt;em>loss function&lt;/em>, &lt;em>risk&lt;/em> and &lt;em>empirical risk minimization&lt;/em> are introduced.&lt;/p></description></item><item><title>Chapter 01.07: Optimization</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/01_ml_basics/01-07-optimization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/01_ml_basics/01-07-optimization/</guid><description>&lt;p>In this section we study parameter optimization as computational solution to machine learning problems. We address pitfalls in non-convex optimization problems and introduce the fundamental concept of gradient descent.&lt;/p></description></item><item><title>Chapter 01.08: Components of a Learner</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/01_ml_basics/01-08-learnercomponents-hro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/01_ml_basics/01-08-learnercomponents-hro/</guid><description>&lt;p>Nearly all supervised learning algorithms can be described in terms of three components: 1) hypothesis space, 2) risk, and 3) optimization. In this section, we explain how these components interact and why this is a very useful concept for many supervised learning approaches.&lt;/p></description></item><item><title>Chapter 2.1: Loss Functions for Regression</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/02_supervised_regression/02-01-loss_functions_for_regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/02_supervised_regression/02-01-loss_functions_for_regression/</guid><description>&lt;p>L1 and L2 are two essential loss functions used for evaluating the performance of regression models. This section defines L1 and L2 loss and explains the differences.&lt;/p></description></item><item><title>Chapter 03.01: Classification Tasks</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/03_supervised_classification/03-01-tasks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/03_supervised_classification/03-01-tasks/</guid><description>&lt;p>In classification, the task is to predict a categorical (binary or multiclass) label. In this section, we illustrate the concept of classification with some typical examples.&lt;/p></description></item><item><title>Chapter 03.02: Basic Definitions</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/03_supervised_classification/03-02-classification-basicdefs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/03_supervised_classification/03-02-classification-basicdefs/</guid><description>&lt;p>Although we are primarily interested in actual class labels, classification models usually output scores or probabilities first. We will explain why, introduce the concepts of decision regions and decision boundaries, and discern two fundamental approaches to constructing classifiers: the generative approach and the discriminant approach.&lt;/p></description></item><item><title>Chapter 03.03: Linear Classifiers</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/03_supervised_classification/03-03-classification-linear/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/03_supervised_classification/03-03-classification-linear/</guid><description>&lt;p>Linear classifiers are an essential subclass of classification models. This section provides the definition of a linear classifier and depicts differences between linear and non-linear decision boundaries.&lt;/p></description></item><item><title>Chapter 03.04: Logistic Regression</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/03_supervised_classification/03-04-classification-logistic/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/03_supervised_classification/03-04-classification-logistic/</guid><description>&lt;p>Logistic regression is a discriminant approach toward constructing a classifier. We will motivate logistic regression via the logistic function, define the log-loss for optimization and illustrate the approach in 1D and 2D.&lt;/p></description></item><item><title>Chapter 03.05: Discriminant Analysis</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/03_supervised_classification/03-05-classification-discranalysis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/03_supervised_classification/03-05-classification-discranalysis/</guid><description>&lt;p>Discriminant analysis is a generative approach toward constructing a classifier. We distinguish between linear (LDA) and quadratic (QDA) discriminant analysis, where the latter is a more flexible approach subsuming the first.&lt;/p></description></item><item><title>Chapter 03.06: Naive Bayes</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/03_supervised_classification/03-06-classification-naivebayes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/03_supervised_classification/03-06-classification-naivebayes/</guid><description>&lt;p>Naive Bayes is a generative approach based on an assumption of conditional independence across features and closely related to discriminant analysis.&lt;/p></description></item><item><title>Chapter 04.01: Generalization Error</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/04_evaluation/04-01-generalization-error/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/04_evaluation/04-01-generalization-error/</guid><description>&lt;p>It is a crucial part of machine learning to evaluate the performance of a learner. We will explain the concept of generalization error and the difference between inner and outer loss.&lt;/p></description></item><item><title>Chapter 04.02: Measures Regression</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/04_evaluation/04-02-measures-regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/04_evaluation/04-02-measures-regression/</guid><description>&lt;p>In this section we familiarize ourselves with essential performance measures for regression. In particular, mean squared error (MSE), mean absolute error (MAE), and a straightforward generalization of $R^2$ are discussed.&lt;/p></description></item><item><title>Chapter 04.03: Training Error</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/04_evaluation/04-03-train/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/04_evaluation/04-03-train/</guid><description>&lt;p>There are two types of errors: training errors and test errors. The focus of this section is on the training error and related difficulties.&lt;/p></description></item><item><title>Chapter 04.04: Test Error</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/04_evaluation/04-04-test/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/04_evaluation/04-04-test/</guid><description>&lt;p>While we can infer some information about the learning process from training errors (e.g., the state of iterative optimization), we are truly interested in generalization ability, and thus in the test error on previously unseen data.&lt;/p></description></item><item><title>Chapter 04.05: Overfitting</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/04_evaluation/04-05-overfitting/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/04_evaluation/04-05-overfitting/</guid><description>&lt;p>When a machine learning model performs well on training data but does not generalize on the test data, we speak of overfitting. We will show you examples of this behavior and how to diagnose overfitting.&lt;/p></description></item><item><title>Chapter 04.06: Resampling 1</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/04_evaluation/04-06-resampling-1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/04_evaluation/04-06-resampling-1/</guid><description>&lt;p>Different resampling techniques help to assess the performance of a learner while avoiding potential quirks resulting from a single train-test split. We will introduce cross-validation (with and without stratification), bootstrap and subsampling.&lt;/p></description></item><item><title>Chapter 04.07: Resampling 2</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/04_evaluation/04-07-resampling-2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/04_evaluation/04-07-resampling-2/</guid><description>### Lecture slides Previous &amp;nbsp; Page: / &amp;nbsp; Next PDF window.addEventListener("load",function(){ var url = 'https:\/\/slds-lmu.github.io\/website_i2ml_copy\/chapters\/04_evaluation\/slides-evaluation-measures-classification.pdf'; var hidePaginator = ""; var pdfjsLib = window['pdfjs-dist/build/pdf']; pdfjsLib.GlobalWorkerOptions.workerSrc = "https://cdn.jsdelivr.net/npm/pdfjs-dist@2.9.359/build/pdf.worker.min.js"; var pdfDoc = null, pageNum = 1, pageRendering = false, pageNumPending = null, scale = 3, canvas_slides_evaluation_measures_classification_pdf = document.getElementById('the-canvas_slides_evaluation_measures_classification_pdf'), ctx = canvas_slides_evaluation_measures_classification_pdf.getContext('2d'); function renderPage(num) { pageRendering = true; pdfDoc.getPage(num).then(function(page) { var viewport = page.getViewport({scale: scale}); canvas_slides_evaluation_measures_classification_pdf.</description></item><item><title>Chapter 04.08: Measures Classification</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/04_evaluation/04-08-measures-classification/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/04_evaluation/04-08-measures-classification/</guid><description>&lt;p>Analogous to regression, we consider essential performance measures for classification. As a classifier predicts either class labels or scores/probabilities, its performance can be evaluated based on these two notions. We show some performance measures for classification, including misclassification error rate (MCE), accuracy (ACC) and Brier score (BS). In addition, we will see confusion matrices and learn about costs.&lt;/p></description></item><item><title>Chapter 04.09: Measures Classification ROC</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/04_evaluation/04-09-measures-classification-roc/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/04_evaluation/04-09-measures-classification-roc/</guid><description>&lt;p>From the confusion matrix we can calculate a variety of ROC metrics. Among others, we will explain true positive rate, negative predictive value and the $F1$ measure.&lt;/p></description></item><item><title>Chapter 04.10: Measures Classification ROC Visualization</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/04_evaluation/04-10-measures-classification-roc-space/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/04_evaluation/04-10-measures-classification-roc-space/</guid><description>&lt;p>In this section, we explain the ROC curve and how to calculate it. In addition, we will present AUC and partial AUC as global performance measures.&lt;/p></description></item><item><title>Chapter 04.11: AUC Extensions</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/04_evaluation/04-11-auc_extensions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/04_evaluation/04-11-auc_extensions/</guid><description>### Lecture slides Previous &amp;nbsp; Page: / &amp;nbsp; Next PDF window.addEventListener("load",function(){ var url = 'https:\/\/slds-lmu.github.io\/website_i2ml_copy\/chapters\/04_evaluation\/slides-evaluation-measures-classification.pdf'; var hidePaginator = ""; var pdfjsLib = window['pdfjs-dist/build/pdf']; pdfjsLib.GlobalWorkerOptions.workerSrc = "https://cdn.jsdelivr.net/npm/pdfjs-dist@2.9.359/build/pdf.worker.min.js"; var pdfDoc = null, pageNum = 1, pageRendering = false, pageNumPending = null, scale = 3, canvas_slides_evaluation_measures_classification_pdf = document.getElementById('the-canvas_slides_evaluation_measures_classification_pdf'), ctx = canvas_slides_evaluation_measures_classification_pdf.getContext('2d'); function renderPage(num) { pageRendering = true; pdfDoc.getPage(num).then(function(page) { var viewport = page.getViewport({scale: scale}); canvas_slides_evaluation_measures_classification_pdf.</description></item><item><title>Chapter 04.12: Beyond AUC</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/04_evaluation/04-12-beyond_auc/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/04_evaluation/04-12-beyond_auc/</guid><description>### Lecture slides Previous &amp;nbsp; Page: / &amp;nbsp; Next PDF window.addEventListener("load",function(){ var url = 'https:\/\/slds-lmu.github.io\/website_i2ml_copy\/chapters\/04_evaluation\/slides-evaluation-measures-classification.pdf'; var hidePaginator = ""; var pdfjsLib = window['pdfjs-dist/build/pdf']; pdfjsLib.GlobalWorkerOptions.workerSrc = "https://cdn.jsdelivr.net/npm/pdfjs-dist@2.9.359/build/pdf.worker.min.js"; var pdfDoc = null, pageNum = 1, pageRendering = false, pageNumPending = null, scale = 3, canvas_slides_evaluation_measures_classification_pdf = document.getElementById('the-canvas_slides_evaluation_measures_classification_pdf'), ctx = canvas_slides_evaluation_measures_classification_pdf.getContext('2d'); function renderPage(num) { pageRendering = true; pdfDoc.getPage(num).then(function(page) { var viewport = page.getViewport({scale: scale}); canvas_slides_evaluation_measures_classification_pdf.</description></item><item><title>Chapter 05.01: k-Nearest Neighbors (k-NN)</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/05_knn/05-01-knn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/05_knn/05-01-knn/</guid><description>&lt;p>We demonstrate that distances in feature space are crucial in \(k\)-NN regression / classification and show how we can form predictions by averaging / majority vote. In this, \(k\)-NN is a very local model and works without distributional assumptions.&lt;/p></description></item><item><title>Chapter 06.01: Introduction</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/06_trees/06-01-intro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/06_trees/06-01-intro/</guid><description>&lt;p>Decision trees are an important type of machine learning model and come in two main types: classification and regression trees. In this section, we explain the general idea of CART and show how they recursively divide up the input space into ever smaller rectangular partitions.&lt;/p></description></item><item><title>Chapter 06.02: Splitting Criteria</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/06_trees/06-02-splitcriteria/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/06_trees/06-02-splitcriteria/</guid><description>&lt;p>CART algorithms require splitting criteria for trees, which are usually defined in terms of impurity reduction. In this section we formalize the idea of splitting criteria and explain the details of splitting for both regression and classification.&lt;/p></description></item><item><title>Chapter 06.03: Growing a Tree</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/06_trees/06-03-treegrowing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/06_trees/06-03-treegrowing/</guid><description>&lt;p>In this section, we explain how to grow a tree starting with an empty tree, i.e., a root node containing all the data. It will be shown that trees are grown by recursively applying greedy optimization to each node.&lt;/p></description></item><item><title>Chapter 06.04: Computational Aspects of Finding Splits</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/06_trees/06-04-splitcomputation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/06_trees/06-04-splitcomputation/</guid><description>&lt;p>In this section, we explain the computational aspects of the node-splitting procedure, especially for nominal features. In addition, we illustrate how to deal with missing values.&lt;/p></description></item><item><title>Chapter 06.05: Stopping Criteria &amp; Pruning</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/06_trees/06-05-stoppingpruning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/06_trees/06-05-stoppingpruning/</guid><description>&lt;p>The recursive partitioning procedure used to grow a CART usually leads to problems such as exponential growth of computations, overfitting, and the horizon effect. To deal with these problems, we can use stopping criteria and pruning. In this section, we explain the basis of these two solutions.&lt;/p></description></item><item><title>Chapter 06.06: Discussion</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/06_trees/06-06-discussion/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/06_trees/06-06-discussion/</guid><description>&lt;p>In this section we discuss the advantages and disadvantages of CART and mention other tree methodologies.&lt;/p></description></item><item><title>Chapter 07.01: Bagging Ensembles</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/07_forests/07-01-bagging/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/07_forests/07-01-bagging/</guid><description>&lt;p>Bagging (bootstrap aggregation) is a method for combining many models into a meta-model which often works much better than its individual components. In this section, we present the basic idea of bagging and explain why and when bagging works.&lt;/p></description></item><item><title>Chapter 07.02: Introduction</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/07_forests/07-02-intro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/07_forests/07-02-intro/</guid><description>&lt;p>In this section we investigate random forests, a modification of bagging for trees. We illustrate the effect of ensemble size and show how to compute out-of-bag error estimates.&lt;/p></description></item><item><title>Chapter 07.03: Benchmarking Trees, Forests, and Bagging k-NN</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/07_forests/07-03-benchmark/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/07_forests/07-03-benchmark/</guid><description>&lt;p>We compare the performance of random forests vs. (bagged) CART and (bagged) \(k\)-NN.&lt;/p></description></item><item><title>Chapter 07.04: Feature Importance</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/07_forests/07-04-featureimportance/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/07_forests/07-04-featureimportance/</guid><description>&lt;p>In a complex machine learning model, the contributions of the different features to the model performance are difficult to evaluate. The concept of feature importance allows to quantify these effects for random forests.&lt;/p></description></item><item><title>Chapter 07.05: Proximities</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/07_forests/07-05-proximities/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/07_forests/07-05-proximities/</guid><description>&lt;p>The term &lt;em>proximity&lt;/em> refers to the &amp;ldquo;closeness&amp;rdquo; between pairs of cases. Proximities are calculated for each pair of observations and can be derived directly from random forests.&lt;/p></description></item><item><title>Chapter 07.06: Discussion</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/07_forests/07-06-discussion/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/07_forests/07-06-discussion/</guid><description>&lt;p>In this section we discuss the advantages and disadvantages of random forests and explain that all advantages of trees also apply here.&lt;/p></description></item><item><title>Chapter 08.01: Introduction</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/08_tuning/08-01-intro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/08_tuning/08-01-intro/</guid><description>&lt;p>While model parameters are optimized during training, hyperparameters must be specified in advance. In this section, we will motivate why it is crucial to find good values for, i.e. to tune, these hyperparameters.&lt;/p></description></item><item><title>Chapter 08.02: Problem Definition</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/08_tuning/08-02-tuning-tuningproblem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/08_tuning/08-02-tuning-tuningproblem/</guid><description>&lt;p>Hyperparameter tuning is the process of finding good model hyperparameters. In this section we formalize the problem of tuning and explain why tuning is computationally hard.&lt;/p></description></item><item><title>Chapter 08.03: Basic Techniques</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/08_tuning/08-03-basicalgos/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/08_tuning/08-03-basicalgos/</guid><description>&lt;p>In this section we familiarize ourselves with two simple but popular tuning strategies, namely grid search and random search, and discuss their advantages and disadvantages.&lt;/p></description></item><item><title>Chapter 09.01: Motivation</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/09_nested_resampling/09-01-nestedintro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/09_nested_resampling/09-01-nestedintro/</guid><description>&lt;p>Selecting the best model from a set of candidates is an important part of most machine learning problems. By examining an instructive and problematic example, we introduce the untouched-test-set principle.&lt;/p></description></item><item><title>Chapter 09.02: Training - Validation - Testing</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/09_nested_resampling/09-02-trainvalidtest/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/09_nested_resampling/09-02-trainvalidtest/</guid><description>&lt;p>The simplest method to achieve an untouched test set is a 3-way split: the models are first trained on the &lt;em>training set&lt;/em> and then evaluated and compared on the &lt;em>validation set&lt;/em>. After selecting the best model, the final performance will be evaluated on the &lt;em>test set&lt;/em>.&lt;/p></description></item><item><title>Chapter 09.03: Nested Resampling</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/09_nested_resampling/09-03-nestedresampling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/09_nested_resampling/09-03-nestedresampling/</guid><description>&lt;p>In this section, we will explain why and how nested resampling is done.&lt;/p></description></item><item><title>Chapter 10.01: Intro to mlr3</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/10_mlr3/10-01-intro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/10_mlr3/10-01-intro/</guid><description>&lt;p>In this section, we introduce the basic concepts of the R package mlr3.&lt;/p></description></item><item><title>Chapter 10.02: Resampling with mlr3</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/10_mlr3/10-02-resampling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/10_mlr3/10-02-resampling/</guid><description>&lt;p>mlr3 supports various forms of resampling, which we will demonstrate in this section.&lt;/p></description></item><item><title>Chapter 10.03: Tuning with mlr3</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/10_mlr3/10-03-tuning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/10_mlr3/10-03-tuning/</guid><description>&lt;p>We can easily conduct hyperparameter tuning with mrl3&amp;rsquo;s modular ecosystem, defining custom search spaces and suitable tuning algorithms.&lt;/p></description></item><item><title>Chapter 10.04: Pipelines with mlr3</title><link>https://slds-lmu.github.io/website_i2ml_copy/chapters/10_mlr3/10-04-pipelines/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/chapters/10_mlr3/10-04-pipelines/</guid><description>&lt;p>Pipelines provide an important tool to package all relevant machine learning steps into a single object that can be treated just like any other learner, ensuring adherence to the train-test split principle even in the face of complex analyses.&lt;/p></description></item><item><title/><link>https://slds-lmu.github.io/website_i2ml_copy/exercises/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/exercises/</guid><description>Exercises Your markdown comes here!
Exercise1: Download &amp;raquo;ex_advriskmin_1.pdf&amp;laquo;</description></item><item><title/><link>https://slds-lmu.github.io/website_i2ml_copy/references/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/references/</guid><description>References Your markdown comes here!</description></item><item><title/><link>https://slds-lmu.github.io/website_i2ml_copy/team/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/team/</guid><description>Team Bernd Bischl wrote the initial version of most of the course material, and teaches various master courses in ML and DL at the LMU for stats and data science. Fabian Scheipl joined the team in fall 2018 and contributed to the slides, videos and code demos. Daniel Schalk is a PhD at Bernd&amp;rsquo;s group and organized the second round of the inverted classroom in spring 2019; he also provided the digital platform for videos, quizzes and exercises.</description></item><item><title>Cheat Sheets</title><link>https://slds-lmu.github.io/website_i2ml_copy/appendix/01_cheat_sheets/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/appendix/01_cheat_sheets/</guid><description> I2ML :: BASICS Download</description></item><item><title>Errata</title><link>https://slds-lmu.github.io/website_i2ml_copy/appendix/02_errata/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/website_i2ml_copy/appendix/02_errata/</guid><description>Errata in the slides shown in the videos Chapter 1.4 (Models &amp;amp; Parameters) - slide 5/10: d-dimensional vector, not p-dimensional Chapter 4.3 (Simple Measures for Classification) - slide 6/9: Error in cost matrix Chapter 5.2 (CART: Splitting Criteria) - slide 12/12: Error in result of Gini Chapter 6.2 (Forests: Intro) - slides 7/8 and 8/8: Error in OOB error Chapter 6.4 (Forests: Feature importance) - slide 3/3: Error in permutation based variable importance</description></item></channel></rss>