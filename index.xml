<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Introduction to Machine Learning (I2ML)</title><link>https://slds-lmu.github.io/i2ml/</link><description>Recent content on Introduction to Machine Learning (I2ML)</description><generator>Hugo</generator><language>en-us</language><atom:link href="https://slds-lmu.github.io/i2ml/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 01.00: ML Basics: In a Nutshell</title><link>https://slds-lmu.github.io/i2ml/chapters/01_ml_basics/01-00-nutshell-basics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/01_ml_basics/01-00-nutshell-basics/</guid><description>&lt;p>In this nutshell chunk, we dive into the foundational principles of Machine Learning.&lt;/p></description></item><item><title>Chapter 01.01: What is ML?</title><link>https://slds-lmu.github.io/i2ml/chapters/01_ml_basics/01-01-what_is_ml/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/01_ml_basics/01-01-what_is_ml/</guid><description>&lt;p>As subtopic of artificial intelligence, machine learning is a mathematically well-defined discipline and usually constructs predictive or decision models from data, instead of explicitly programming them. In this section, you will see some typical examples of where machine learning is applied and the main directions of machine learning.&lt;/p></description></item><item><title>Chapter 01.02: Data</title><link>https://slds-lmu.github.io/i2ml/chapters/01_ml_basics/01-02-data/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/01_ml_basics/01-02-data/</guid><description>&lt;p>In this section we explain the basic structure of tabular data used in machine learning. We will differentiate targets from features, talk about labeled and unlabeled data and introduce the concept of the data generating process.&lt;/p></description></item><item><title>Chapter 01.03: Tasks</title><link>https://slds-lmu.github.io/i2ml/chapters/01_ml_basics/01-03-tasks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/01_ml_basics/01-03-tasks/</guid><description>&lt;p>The tasks of supervised learning can roughly be divided in two categories: regression (for continuous outcome) and classification (for categorical outcome). We will present some examples.&lt;/p></description></item><item><title>Chapter 01.04: Models and Parameters</title><link>https://slds-lmu.github.io/i2ml/chapters/01_ml_basics/01-04-models-parameters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/01_ml_basics/01-04-models-parameters/</guid><description>&lt;p>We introduce models as functional hypotheses about the mapping from feature to target space that allow us to make predictions by computing a function of the input data. Frequently in machine learning, models are understood to be parameterized curves, which is illustrated by several examples.&lt;/p></description></item><item><title>Chapter 01.05: Learner</title><link>https://slds-lmu.github.io/i2ml/chapters/01_ml_basics/01-05-learner/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/01_ml_basics/01-05-learner/</guid><description>&lt;p>Roughly speaking, learners (endowed with a specific hyperparameter configuration) take training data and return a model.&lt;/p></description></item><item><title>Chapter 01.06: Losses and Risk Minimization</title><link>https://slds-lmu.github.io/i2ml/chapters/01_ml_basics/01-06-riskminimization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/01_ml_basics/01-06-riskminimization/</guid><description>&lt;p>In order to find good solutions we need a concept to evaluate and compare models. To this end, the concepts of &lt;em>loss function&lt;/em>, &lt;em>risk&lt;/em> and &lt;em>empirical risk minimization&lt;/em> are introduced.&lt;/p></description></item><item><title>Chapter 01.07: Optimization</title><link>https://slds-lmu.github.io/i2ml/chapters/01_ml_basics/01-07-optimization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/01_ml_basics/01-07-optimization/</guid><description>&lt;p>In this section we study parameter optimization as computational solution to machine learning problems. We address pitfalls in non-convex optimization problems and introduce the fundamental concept of gradient descent.&lt;/p></description></item><item><title>Chapter 01.08: Components of a Learner</title><link>https://slds-lmu.github.io/i2ml/chapters/01_ml_basics/01-08-learnercomponents-hro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/01_ml_basics/01-08-learnercomponents-hro/</guid><description>&lt;p>Nearly all supervised learning algorithms can be described in terms of three components: 1) hypothesis space, 2) risk, and 3) optimization. In this section, we explain how these components interact and why this is a very useful concept for many supervised learning approaches.&lt;/p></description></item><item><title>Chapter 02.00: Supervised Regression: In a Nutshell</title><link>https://slds-lmu.github.io/i2ml/chapters/02_supervised_regression/02-00-nutshell-regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/02_supervised_regression/02-00-nutshell-regression/</guid><description>&lt;p>In this nutshell chunk, we explore the fundamentals of supervised regression, where we teach machines to predict continuous outcomes based on input data.&lt;/p></description></item><item><title>Chapter 02.01: Linear Models with L2 Loss</title><link>https://slds-lmu.github.io/i2ml/chapters/02_supervised_regression/02-01-l2-loss/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/02_supervised_regression/02-01-l2-loss/</guid><description>&lt;p>In this section, we focus on the general concept of linear regression and explain how the linear regression model can be used from a machine learning perspective to predict a continuous numerical target variable. Furthermore, we introduce the \(L2\) loss in the context of linear regression and explain how its use results in an SSE-minimal model.&lt;/p></description></item><item><title>Chapter 02.02: Proof OLS Regression: Deep Dive</title><link>https://slds-lmu.github.io/i2ml/chapters/02_supervised_regression/02-02-ols/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/02_supervised_regression/02-02-ols/</guid><description>&lt;p>In this section, we provide you with a proof for the ordinary least squares (OLS) method.&lt;/p></description></item><item><title>Chapter 02.03: Linear Models with L1 Loss</title><link>https://slds-lmu.github.io/i2ml/chapters/02_supervised_regression/02-03-l1-loss/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/02_supervised_regression/02-03-l1-loss/</guid><description>&lt;p>In this section, we introduce \(L1\) loss and elaborate its differences to \(L2\) loss. In addition, we explain how the choice of loss affects optimization and robustness.&lt;/p></description></item><item><title>Chapter 02.04: Polynomial Regression Models</title><link>https://slds-lmu.github.io/i2ml/chapters/02_supervised_regression/02-04-polynomials/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/02_supervised_regression/02-04-polynomials/</guid><description>&lt;p>This section introduces polynomials to obtain more flexible models for the regression task. We explain the connection to the basic linear model and discuss the problem of overfitting.&lt;/p></description></item><item><title>Chapter 03.00: Supervised Classification: In a Nutshell</title><link>https://slds-lmu.github.io/i2ml/chapters/03_supervised_classification/03-00-nutshell-classification/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/03_supervised_classification/03-00-nutshell-classification/</guid><description>&lt;p>In this nutshell chunk, we delve into the basics of supervised classification, where we train machines to categorize input data into predefined labels.&lt;/p></description></item><item><title>Chapter 03.01: Classification Tasks</title><link>https://slds-lmu.github.io/i2ml/chapters/03_supervised_classification/03-01-tasks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/03_supervised_classification/03-01-tasks/</guid><description>&lt;p>In classification, the task is to predict a categorical (binary or multiclass) label. In this section, we illustrate the concept of classification with some typical examples.&lt;/p></description></item><item><title>Chapter 03.02: Basic Definitions</title><link>https://slds-lmu.github.io/i2ml/chapters/03_supervised_classification/03-02-classification-basicdefs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/03_supervised_classification/03-02-classification-basicdefs/</guid><description>&lt;p>Although we are primarily interested in actual class labels, classification models usually output scores or probabilities first. We will explain why, introduce the concepts of decision regions and decision boundaries, and discern two fundamental approaches to constructing classifiers: the generative approach and the discriminant approach.&lt;/p></description></item><item><title>Chapter 03.03: Linear Classifiers</title><link>https://slds-lmu.github.io/i2ml/chapters/03_supervised_classification/03-03-classification-linear/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/03_supervised_classification/03-03-classification-linear/</guid><description>&lt;p>Linear classifiers are an essential subclass of classification models. This section provides the definition of a linear classifier and depicts differences between linear and non-linear decision boundaries.&lt;/p></description></item><item><title>Chapter 03.04: Logistic Regression</title><link>https://slds-lmu.github.io/i2ml/chapters/03_supervised_classification/03-04-classification-logistic/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/03_supervised_classification/03-04-classification-logistic/</guid><description>&lt;p>Logistic regression is a discriminant approach toward constructing a classifier. We will motivate logistic regression via the logistic function, define the log-loss for optimization and illustrate the approach in 1D and 2D.&lt;/p></description></item><item><title>Chapter 03.05: Discriminant Analysis</title><link>https://slds-lmu.github.io/i2ml/chapters/03_supervised_classification/03-05-classification-discranalysis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/03_supervised_classification/03-05-classification-discranalysis/</guid><description>&lt;p>Discriminant analysis is a generative approach toward constructing a classifier. We distinguish between linear (LDA) and quadratic (QDA) discriminant analysis, where the latter is a more flexible approach subsuming the first.&lt;/p></description></item><item><title>Chapter 03.06: Naive Bayes</title><link>https://slds-lmu.github.io/i2ml/chapters/03_supervised_classification/03-06-classification-naivebayes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/03_supervised_classification/03-06-classification-naivebayes/</guid><description>&lt;p>Naive Bayes is a generative approach based on an assumption of conditional independence across features and closely related to discriminant analysis.&lt;/p></description></item><item><title>Chapter 04.00: Evaluation: In a Nutshell</title><link>https://slds-lmu.github.io/i2ml/chapters/04_evaluation/04-00-nutshell-evaluation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/04_evaluation/04-00-nutshell-evaluation/</guid><description>&lt;p>In this nutshell chunk, we delve into the critical aspects of evaluation, unraveling how we measure and ensure the effectiveness and accuracy of machine learning models.&lt;/p></description></item><item><title>Chapter 04.01: Generalization Error</title><link>https://slds-lmu.github.io/i2ml/chapters/04_evaluation/04-01-generalization-error/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/04_evaluation/04-01-generalization-error/</guid><description>&lt;p>It is a crucial part of machine learning to evaluate the performance of a learner. We will explain the concept of generalization error and the difference between inner and outer loss.&lt;/p></description></item><item><title>Chapter 04.02: Measures Regression</title><link>https://slds-lmu.github.io/i2ml/chapters/04_evaluation/04-02-measures-regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/04_evaluation/04-02-measures-regression/</guid><description>&lt;p>In this section we familiarize ourselves with essential performance measures for regression. In particular, mean squared error (MSE), mean absolute error (MAE), and a straightforward generalization of $R^2$ are discussed.&lt;/p></description></item><item><title>Chapter 04.03: Training Error</title><link>https://slds-lmu.github.io/i2ml/chapters/04_evaluation/04-03-train/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/04_evaluation/04-03-train/</guid><description>&lt;p>There are two types of errors: training errors and test errors. The focus of this section is on the training error and related difficulties.&lt;/p></description></item><item><title>Chapter 04.04: Test Error</title><link>https://slds-lmu.github.io/i2ml/chapters/04_evaluation/04-04-test/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/04_evaluation/04-04-test/</guid><description>&lt;p>While we can infer some information about the learning process from training errors (e.g., the state of iterative optimization), we are truly interested in generalization ability, and thus in the test error on previously unseen data.&lt;/p></description></item><item><title>Chapter 04.05: Overfitting &amp; Underfitting</title><link>https://slds-lmu.github.io/i2ml/chapters/04_evaluation/04-05-overfitting-underfitting/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/04_evaluation/04-05-overfitting-underfitting/</guid><description>&lt;p>In machine learning, we are interested in a model that captures the true underlying function and still generalizes well to new data.
When the model fails on the first task, we speak of underfitting, and both train and test error will be high.
On the other hand, learning the training data very well at the expense of generalization ability is referred to as overfitting and usually occurs when there is not enough data to tell our hypotheses apart.
We will show you examples of this behavior and how to diagnose overfitting.&lt;/p></description></item><item><title>Chapter 04.06: Resampling 1</title><link>https://slds-lmu.github.io/i2ml/chapters/04_evaluation/04-06-resampling-1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/04_evaluation/04-06-resampling-1/</guid><description>&lt;p>Different resampling techniques help to assess the performance of a learner while avoiding potential quirks resulting from a single train-test split. We will introduce cross-validation (with and without stratification), bootstrap and subsampling.&lt;/p></description></item><item><title>Chapter 04.07: Resampling 2</title><link>https://slds-lmu.github.io/i2ml/chapters/04_evaluation/04-07-resampling-2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/04_evaluation/04-07-resampling-2/</guid><description>&lt;p>We provide a deep dive on resampling, showing its superiority to holdout
splitting and analyzing the bias-variance decomposition of its MSE.
We further point out the dependence between CV fold results and that
hypothesis testing is therefore not applicable, and give some practical tips
to choose resampling strategies.&lt;/p></description></item><item><title>Chapter 04.08: Measures Classification</title><link>https://slds-lmu.github.io/i2ml/chapters/04_evaluation/04-08-measures-classification/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/04_evaluation/04-08-measures-classification/</guid><description>&lt;p>Analogous to regression, we consider essential performance measures for classification. As a classifier predicts either class labels or scores/probabilities, its performance can be evaluated based on these two notions. We show some performance measures for classification, including misclassification error rate (MCE), accuracy (ACC) and Brier score (BS). In addition, we will see confusion matrices and learn about costs.&lt;/p></description></item><item><title>Chapter 04.09: ROC Basics</title><link>https://slds-lmu.github.io/i2ml/chapters/04_evaluation/04-09-rocbasics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/04_evaluation/04-09-rocbasics/</guid><description>&lt;p>From the confusion matrix we can calculate a variety of ROC metrics. Among others, we will explain true positive rate, negative predictive value and the $F1$ measure.&lt;/p></description></item><item><title>Chapter 04.10: ROC Curves</title><link>https://slds-lmu.github.io/i2ml/chapters/04_evaluation/04-10-roccurves/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/04_evaluation/04-10-roccurves/</guid><description>&lt;p>In this section, we explain the ROC curve and how to calculate it. In addition, we will present the AUC as a global performance measure that integrates over all possible thresholds.&lt;/p></description></item><item><title>Chapter 04.11: Partial AUC &amp; Multi-Class AUC</title><link>https://slds-lmu.github.io/i2ml/chapters/04_evaluation/04-11-partialauc-mcauc/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/04_evaluation/04-11-partialauc-mcauc/</guid><description>&lt;p>We discuss both the partial AUC, which restricts the AUC to the relevant area
for a specific application, and possible extensions of the AUC to multi-class
classification.&lt;/p></description></item><item><title>Chapter 04.12: Precision-Recall Curves</title><link>https://slds-lmu.github.io/i2ml/chapters/04_evaluation/04-12-prcurves/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/04_evaluation/04-12-prcurves/</guid><description>&lt;p>Besides plotting TPR against FPR to obtain the ROC curve, it sometimes makes
sense to instead consider precision (= PPV) vs recall (= TPR), especially when
data are imbalanced.&lt;/p></description></item><item><title>Chapter 04.13: AUC &amp; Mann-Whitney-U Test</title><link>https://slds-lmu.github.io/i2ml/chapters/04_evaluation/04-13-auc-mwu/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/04_evaluation/04-13-auc-mwu/</guid><description>&lt;p>We demonstrate that the AUC is equivalent to the normalized test statistic in
the Mann-Whitney-U test, both of which are effectively rank-based metrics.&lt;/p></description></item><item><title>Chapter 05.01: k-Nearest Neighbors (k-NN)</title><link>https://slds-lmu.github.io/i2ml/chapters/05_knn/05-01-knn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/05_knn/05-01-knn/</guid><description>&lt;p>We demonstrate that distances in feature space are crucial in \(k\)-NN regression / classification and show how we can form predictions by averaging / majority vote. In this, \(k\)-NN is a very local model and works without distributional assumptions.&lt;/p></description></item><item><title>Chapter 06.00: CART: In a Nutshell</title><link>https://slds-lmu.github.io/i2ml/chapters/06_cart/06-00-nutshell-cart/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/06_cart/06-00-nutshell-cart/</guid><description>&lt;p>In this nutshell chunk, we unravel the workings of CARTs (Classification and Regression Trees).&lt;/p></description></item><item><title>Chapter 06.01: Predictions with CART</title><link>https://slds-lmu.github.io/i2ml/chapters/06_cart/06-01-predictions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/06_cart/06-01-predictions/</guid><description>&lt;p>Decision trees are an important type of machine learning model and come in two main types: classification and regression trees. In this section, we explain the general idea of CART and show how they recursively divide up the input space into ever smaller rectangular partitions.
Thus, we think of CART for now only as a predictor.&lt;/p></description></item><item><title>Chapter 06.02: Growing a Tree</title><link>https://slds-lmu.github.io/i2ml/chapters/06_cart/06-02-treegrowing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/06_cart/06-02-treegrowing/</guid><description>&lt;p>In this section, we explain how to grow a tree starting with an empty tree, i.e., a root node containing all the data. It will be shown that trees are grown by recursively applying greedy optimization to each node.&lt;/p></description></item><item><title>Chapter 06.03: Splitting Criteria for Regression</title><link>https://slds-lmu.github.io/i2ml/chapters/06_cart/06-03-splitcriteria-regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/06_cart/06-03-splitcriteria-regression/</guid><description>&lt;p>CART algorithms require splitting criteria for trees, which are usually defined in terms of impurity reduction. In this section we formalize the idea of splitting criteria and explain the details of splitting. We start with regression and doing so we show how split criteria fit into our framework of empirical risk minimization.&lt;/p></description></item><item><title>Chapter 06.04: Splitting Criteria for Classification</title><link>https://slds-lmu.github.io/i2ml/chapters/06_cart/06-04-splitcriteria-classification/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/06_cart/06-04-splitcriteria-classification/</guid><description>&lt;p>We extend splitting criteria to classification task. Here, we see that there are analogies of ERM and impurity reduction. While these analogies are interested, proving the equivalence of ERM and impurity reduction are beyond the scope of this lecture. The interested reader can refer to chapter 11 of this lecture, where we proof the equivalence.&lt;/p></description></item><item><title>Chapter 06.05: Computational Aspects of Finding Splits</title><link>https://slds-lmu.github.io/i2ml/chapters/06_cart/06-05-computationalaspects/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/06_cart/06-05-computationalaspects/</guid><description>&lt;p>In this section, we explain the computational aspects of the node-splitting procedure, especially for nominal features. In addition, we illustrate how to deal with missing values.&lt;/p></description></item><item><title>Chapter 06.06: Stopping Criteria &amp; Pruning</title><link>https://slds-lmu.github.io/i2ml/chapters/06_cart/06-06-stoppingpruning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/06_cart/06-06-stoppingpruning/</guid><description>&lt;p>The recursive partitioning procedure used to grow a CART usually leads to problems such as exponential growth of computations, overfitting, and the horizon effect. To deal with these problems, we can use stopping criteria and pruning. In this section, we explain the basis of these two solutions.&lt;/p></description></item><item><title>Chapter 06.07: Discussion</title><link>https://slds-lmu.github.io/i2ml/chapters/06_cart/06-07-discussion/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/06_cart/06-07-discussion/</guid><description>&lt;p>In this section we discuss the advantages and disadvantages of CART and mention other tree methodologies.&lt;/p></description></item><item><title>Chapter 07.00: Random Forests: In a Nutshell</title><link>https://slds-lmu.github.io/i2ml/chapters/07_forests/07-00-nutshell-random-forest/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/07_forests/07-00-nutshell-random-forest/</guid><description>&lt;p>In this nutshell chunk, we delve into Random Forests, an ensemble method that harnesses multiple decision trees for improved prediction accuracy and robustness.&lt;/p></description></item><item><title>Chapter 07.01: Bagging Ensembles</title><link>https://slds-lmu.github.io/i2ml/chapters/07_forests/07-01-bagging/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/07_forests/07-01-bagging/</guid><description>&lt;p>Bagging (bootstrap aggregation) is a method for combining many models into a meta-model which often works much better than its individual components. In this section, we present the basic idea of bagging and explain why and when bagging works.&lt;/p></description></item><item><title>Chapter 07.02: Basics</title><link>https://slds-lmu.github.io/i2ml/chapters/07_forests/07-02-basics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/07_forests/07-02-basics/</guid><description>&lt;p>In this section we investigate random forests, a modification of bagging for trees.&lt;/p></description></item><item><title>Chapter 07.03: Out-of-Bag Error Estimate</title><link>https://slds-lmu.github.io/i2ml/chapters/07_forests/07-03-oob-error/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/07_forests/07-03-oob-error/</guid><description>&lt;p>We introduce the concepts of in-bag and out-of-bag observations and explain how to compute the out-of-bag error estimate.&lt;/p></description></item><item><title>Chapter 07.04: Feature Importance</title><link>https://slds-lmu.github.io/i2ml/chapters/07_forests/07-04-featureimportance/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/07_forests/07-04-featureimportance/</guid><description>&lt;p>In a complex machine learning model, the contributions of the different features to the model performance are difficult to evaluate. The concept of feature importance allows to quantify these effects for random forests.&lt;/p></description></item><item><title>Chapter 07.05: Proximities</title><link>https://slds-lmu.github.io/i2ml/chapters/07_forests/07-05-proximities/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/07_forests/07-05-proximities/</guid><description>&lt;p>The term &lt;em>proximity&lt;/em> refers to the &amp;ldquo;closeness&amp;rdquo; between pairs of cases. Proximities are calculated for each pair of observations and can be derived directly from random forests.&lt;/p></description></item><item><title>Chapter 08.00: Neural Networks: In a Nutshell</title><link>https://slds-lmu.github.io/i2ml/chapters/08_neural_networks/08-00-nutshell-nn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/08_neural_networks/08-00-nutshell-nn/</guid><description>&lt;p>In this nutshell chunk, we learn about neural networks, the driving force behind many of today&amp;rsquo;s cutting-edge machine learning applications.&lt;/p></description></item><item><title>Chapter 08.01: Introduction</title><link>https://slds-lmu.github.io/i2ml/chapters/08_neural_networks/08-01-intro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/08_neural_networks/08-01-intro/</guid><description>&lt;p>In this section, we introduces the relationship of DL and ML, give basic intro about feature learning, and discuss the use-cases and data types for DL methods.&lt;/p></description></item><item><title>Chapter 08.02: Single Neuron</title><link>https://slds-lmu.github.io/i2ml/chapters/08_neural_networks/08-02-single-neuron/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/08_neural_networks/08-02-single-neuron/</guid><description>&lt;p>In this section we explain the graphical representation of a single neuron and describe affine transformations and non-linear activation functions. Moreover, we talk about the hypothesis spaces of a single neuron and name some typical loss functions.&lt;/p></description></item><item><title>Chapter 08.03: Single Hidden Layer NN</title><link>https://slds-lmu.github.io/i2ml/chapters/08_neural_networks/08-03-single-hidden-layer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/08_neural_networks/08-03-single-hidden-layer/</guid><description>&lt;p>We introduce architecture of single hidden layer neural networks and discuss the advantage of hidden layers. Then, we explain the typical (non-linear) activation
functions.&lt;/p></description></item><item><title>Chapter 08.04: Single Hidden Layer Networks for Multi-Class Classification</title><link>https://slds-lmu.github.io/i2ml/chapters/08_neural_networks/08-04-nn4multiclass/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/08_neural_networks/08-04-nn4multiclass/</guid><description>&lt;p>In this section, we discuss a neural network architectures for multi-class classification, softmax activation function as well as the Softmax loss.&lt;/p></description></item><item><title>Chapter 08.05: MLP: Multi-Layer Feedforward Neural Networks</title><link>https://slds-lmu.github.io/i2ml/chapters/08_neural_networks/08-05-mulitlayernn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/08_neural_networks/08-05-mulitlayernn/</guid><description>&lt;p>Architectures of deep neural networks and deep neural networks as chained functions are the learning goal of this part.&lt;/p></description></item><item><title>Extra: Brief History</title><link>https://slds-lmu.github.io/i2ml/chapters/08_neural_networks/08-06-history/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/08_neural_networks/08-06-history/</guid><description>&lt;p>We overview history of DL development.&lt;/p></description></item><item><title>Extra: Basic Backpropagation 1</title><link>https://slds-lmu.github.io/i2ml/chapters/08_neural_networks/08-07-backprob1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/08_neural_networks/08-07-backprob1/</guid><description>&lt;p>This section introduces forward and backward passes, chain rule, and the details of backprop in deep learning.&lt;/p></description></item><item><title>Chapter 09.00: Tuning &amp; Nested Resampling: In a Nutshell</title><link>https://slds-lmu.github.io/i2ml/chapters/09_tuning/09-00-nutshell-tuning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/09_tuning/09-00-nutshell-tuning/</guid><description>&lt;p>In this nutshell chunk, we explore tuning and nested resampling, focusing on their roles in evaluating and optimizing the performance of machine learning models.&lt;/p></description></item><item><title>Chapter 09.01: Introduction</title><link>https://slds-lmu.github.io/i2ml/chapters/09_tuning/09-01-intro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/09_tuning/09-01-intro/</guid><description>&lt;p>While model parameters are optimized during training, hyperparameters must be specified in advance. In this section, we will motivate why it is crucial to find good values for, i.e. to tune, these hyperparameters.&lt;/p></description></item><item><title>Chapter 09.02: Problem Definition</title><link>https://slds-lmu.github.io/i2ml/chapters/09_tuning/09-02-tuning-tuningproblem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/09_tuning/09-02-tuning-tuningproblem/</guid><description>&lt;p>Hyperparameter tuning is the process of finding good model hyperparameters. In this section we formalize the problem of tuning and explain why tuning is computationally hard.&lt;/p></description></item><item><title>Chapter 09.03: Basic Techniques</title><link>https://slds-lmu.github.io/i2ml/chapters/09_tuning/09-03-basicalgos/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/09_tuning/09-03-basicalgos/</guid><description>&lt;p>In this section we familiarize ourselves with two simple but popular tuning strategies, namely grid search and random search, and discuss their advantages and disadvantages.&lt;/p></description></item><item><title>Chapter 09.04: Advanced Tuning Techniques</title><link>https://slds-lmu.github.io/i2ml/chapters/09_tuning/09-04-tuning-advanced/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/09_tuning/09-04-tuning-advanced/</guid><description>&lt;p>Besides grid search and random search there are several more advanced techniques for hyperparameter optimization. In this section we focus on model based optimization methods such as Bayesian optimization. Furthermore, we look into multi-fidelity methods such as the hyperband algorithm.&lt;/p></description></item><item><title>Chapter 09.05: Pipelines and AutoML</title><link>https://slds-lmu.github.io/i2ml/chapters/09_tuning/09-05-tuning-pipelines/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/09_tuning/09-05-tuning-pipelines/</guid><description>&lt;p>Some aspects of the machine learning lifecycle can be automated via
AutoML. In this section we look into pipelines as part of AutoML and how (HPO-) pipelines can be represented as directed acyclic graphs (DAGs).&lt;/p></description></item><item><title>Tuning: Further Material</title><link>https://slds-lmu.github.io/i2ml/chapters/09_tuning/further-material/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/09_tuning/further-material/</guid><description/></item><item><title>Chapter 10.01: Motivation</title><link>https://slds-lmu.github.io/i2ml/chapters/10_nested_resampling/10-01-nestedintro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/10_nested_resampling/10-01-nestedintro/</guid><description>&lt;p>Selecting the best model from a set of candidates is an important part of most machine learning problems. By examining an instructive and problematic example, we introduce the untouched-test-set principle.&lt;/p></description></item><item><title>Chapter 10.02: Training - Validation - Testing</title><link>https://slds-lmu.github.io/i2ml/chapters/10_nested_resampling/10-02-trainvalidtest/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/10_nested_resampling/10-02-trainvalidtest/</guid><description>&lt;p>The simplest method to achieve an untouched test set is a 3-way split: the models are first trained on the &lt;em>training set&lt;/em> and then evaluated and compared on the &lt;em>validation set&lt;/em>. After selecting the best model, the final performance will be evaluated on the &lt;em>test set&lt;/em>.&lt;/p></description></item><item><title>Chapter 10.03: Nested Resampling</title><link>https://slds-lmu.github.io/i2ml/chapters/10_nested_resampling/10-03-nestedresampling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/10_nested_resampling/10-03-nestedresampling/</guid><description>&lt;p>In this section, we will explain why and how nested resampling is done.&lt;/p></description></item><item><title>Chapter 11.01: Risk Minimizers</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-01-risk-minimizer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-01-risk-minimizer/</guid><description>&lt;p>We introduce important concepts in theoretical risk minimization: risk minimizer, Bayes risk, Bayes regret, consistent learners and the optimal constant model.&lt;/p></description></item><item><title>Chapter 11.02: Pseudo-Residuals</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-02-pseudo-residuals/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-02-pseudo-residuals/</guid><description>&lt;p>We introduce the concept of pseudo-residuals, i.e., loss residuals in function space, and discuss their relation to gradient descent.&lt;/p></description></item><item><title>Chapter 11.03: L2 Loss</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-03-regression-l2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-03-regression-l2/</guid><description>&lt;p>In this section, we revisit \(L2\) loss and derive its risk minimizer &amp;ndash; the conditional mean &amp;ndash; and optimal constant model &amp;ndash; the empirical mean of observed target values.&lt;/p></description></item><item><title>Chapter 11.04: L1 Loss</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-04-regression-l1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-04-regression-l1/</guid><description>&lt;p>In this section, we revisit \(L1\) loss and derive its risk minimizer &amp;ndash; the conditional median &amp;ndash; and optimal constant model &amp;ndash; the empirical median of observed target values.&lt;/p></description></item><item><title>Chapter 11.05: Advanced Regression Losses</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-05-regression-further-losses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-05-regression-further-losses/</guid><description>&lt;p>In this section, we introduce and discuss the following advanced regression losses: Huber, log-cosh, Cauchy, log-barrier, epsilon-insensitive, and quantile loss.&lt;/p></description></item><item><title>Chapter 11.06: 0-1 Loss</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-06-classification-01/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-06-classification-01/</guid><description>&lt;p>In this section, we revisit the 0-1 loss and derive its risk minimizer .&lt;/p></description></item><item><title>Chapter 11.07: Bernoulli Loss</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-07-classification-bernoulli/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-07-classification-bernoulli/</guid><description>&lt;p>We study the Bernoulli loss and derive its risk minimizer and optimal constant model. We further discuss the connection between Bernoulli loss minimization and tree splitting according to the entropy criterion.&lt;/p></description></item><item><title>Chapter 11.08: Logistic Regression: Deep Dive</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-08-classification-logreg-deep-dive/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-08-classification-logreg-deep-dive/</guid><description>&lt;p>In this segment, we derive the gradient and Hessian of logistice regression and show that logistic regression is a convex problem. This section is presented as a &lt;strong>deep-dive&lt;/strong>. Please note that there are no videos accompanying this section.&lt;/p></description></item><item><title>Chapter 11.09: Brier Score</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-09-classification-brier/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-09-classification-brier/</guid><description>&lt;p>In this section, we introduce the Brier score and derive its risk minimizer and optimal constant model. We further discuss the connection between Brier score minimization and tree splitting according to the Gini index.&lt;/p></description></item><item><title>Chapter 11.10: Advanced Classification Losses</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-10-classification-further-losses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-10-classification-further-losses/</guid><description>&lt;p>In this section, we introduce and discuss the following advanced classification losses: (squared) hinge loss, \(L2\) loss on scores, exponential loss, and AUC loss.&lt;/p></description></item><item><title>Chapter 11.11: Optimal constant model for the empirical log loss risk</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-11-classification-deep-dive/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-11-classification-deep-dive/</guid><description>&lt;p>In this segment, we explore the derivation of the optimal constant model concerning the empirical log loss risk. This section is presented as a &lt;strong>deep-dive&lt;/strong>. Please note that there are no videos accompanying this section.&lt;/p></description></item><item><title>Chapter 11.12: Maximum Likelihood Estimation vs Empirical Risk Minimization I</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-12-max-likelihood-l2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-12-max-likelihood-l2/</guid><description>&lt;p>We discuss the connection between maximum likelihood estimation and risk minimization, then demonstrate the correspondence between a Gaussian error distribution and \(L2\) loss.&lt;/p></description></item><item><title>Chapter 11.13: Maximum Likelihood Estimation vs Empirical Risk Minimization II</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-13-max-likelihood-other/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-13-max-likelihood-other/</guid><description>&lt;p>We discuss the connection between maximum likelihood estimation and risk minimization for further losses (\(L1\) loss, Bernoulli loss).&lt;/p></description></item><item><title>Chapter 11.14: Properties of Loss Functions</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-14-losses-properties/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-14-losses-properties/</guid><description>&lt;p>We discuss the concept of robustness, analytical and functional properties of loss functions and how they may influence the convergence of optimizers.&lt;/p></description></item><item><title>Chapter 11.15: Bias Variance Decomposition</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-15-bias-variance-decomposition/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-15-bias-variance-decomposition/</guid><description>&lt;p>We discuss how to decompose the generalization error of a learner.&lt;/p></description></item><item><title>Chapter 11.16: Bias Variance Decomposition: Deep Dive</title><link>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-16-bias-variance-deep-dive/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/11_advriskmin/11-16-bias-variance-deep-dive/</guid><description>&lt;p>In this segment, we discuss details of the decomposition of the generalization error of a learner. This section is presented as a &lt;strong>deep-dive&lt;/strong>. Please note that there are no videos accompanying this section.&lt;/p></description></item><item><title>Chapter 12.01: Multiclass Classification and Losses</title><link>https://slds-lmu.github.io/i2ml/chapters/12_multiclass/12-01-losses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/12_multiclass/12-01-losses/</guid><description>&lt;p>In this section, we introduce the basic concepts in multiclass (MC) classification and important MC losses: MC 0-1 loss, MC brier score, and MC logarithmic loss.&lt;/p></description></item><item><title>Chapter 12.02: Softmax Regression</title><link>https://slds-lmu.github.io/i2ml/chapters/12_multiclass/12-02-softmax-regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/12_multiclass/12-02-softmax-regression/</guid><description>&lt;p>In this section, we introduce softmax regression as a generalization of logistic regression.&lt;/p></description></item><item><title>Chapter 12.03: One-vs-One and One-vs-Rest</title><link>https://slds-lmu.github.io/i2ml/chapters/12_multiclass/12-03-binary-reduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/12_multiclass/12-03-binary-reduction/</guid><description>&lt;p>It is sometimes advisable to address a multiclass problem as a set of binary ones. We discuss two ways to reduce a multiclass problem to multiple binary classification problems: one-vs-one and one-vs-rest.&lt;/p></description></item><item><title>Chapter 12.04: Designing Codebooks and ECOC</title><link>https://slds-lmu.github.io/i2ml/chapters/12_multiclass/12-04-codebooks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/12_multiclass/12-04-codebooks/</guid><description>&lt;p>In this section, we introduce codebooks as a general concept for multiclass-to- binary reduction.&lt;/p></description></item><item><title>Chapter 13.01: Entropy I</title><link>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-01-entropy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-01-entropy/</guid><description>&lt;p>We introduce entropy, which expresses the expected information for discrete random variables, as a central concept in information theory.&lt;/p></description></item><item><title>Chapter 13.02: Entropy II</title><link>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-02-entropy2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-02-entropy2/</guid><description>&lt;p>We continue our discussion about entropy and introduce joint entropy, the uniqueness theorem and the maximum entropy principle.&lt;/p></description></item><item><title>Chapter 13.03: Differential Entropy</title><link>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-03-diffent/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-03-diffent/</guid><description>&lt;p>In this section, we extend the definition of entropy to the continuous case.&lt;/p></description></item><item><title>Chapter 13.04: Kullback-Leibler Divergence</title><link>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-04-kl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-04-kl/</guid><description>&lt;p>The Kullback-Leibler divergence (KL) is an important quantity for measuring the difference between two probability distributions. We discuss different intuitions for KL and relate it to risk minimization and likelihood ratios.&lt;/p></description></item><item><title>Chapter 13.05: Cross-Entropy and KL</title><link>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-05-cross-entropy-kld/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-05-cross-entropy-kld/</guid><description>&lt;p>We introduce cross-entropy as a further information-theoretic concept and discuss the connection between entropy, cross-entropy, and Kullback-Leibler divergence.&lt;/p></description></item><item><title>Chapter 13.06: Information Theory for Machine Learning</title><link>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-06-ml/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-06-ml/</guid><description>&lt;p>In this section, we discuss how information-theoretic concepts are used in machine learning and demonstrate the equivalence of KL minimization and maximum likelihood maximization, as well as how (cross-)entropy can be used as a loss function.&lt;/p></description></item><item><title>Chapter 13.07: Joint Entropy and Mutual Information I</title><link>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-07-mutual-info/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-07-mutual-info/</guid><description>&lt;p>Information theory also provides means of quantifying relations between two random variables that extend the concept of (linear) correlation. We discuss joint entropy, conditional entropy, and mutual information in this context.&lt;/p></description></item><item><title>Chapter 13.08: Joint Entropy and Mutual Information II</title><link>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-08-mutual-info2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-08-mutual-info2/</guid><description>&lt;p>Information theory also provides means of quantifying relations between two random variables that extend the concept of (linear) correlation. We discuss joint entropy, conditional entropy, and mutual information in this context.&lt;/p></description></item><item><title>Chapter 13.09: Entropy and Optimal Code Length I</title><link>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-09-sourcecoding/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-09-sourcecoding/</guid><description>&lt;p>In this section, we introduce source coding and discuss how entropy can be understood as optimal code length.&lt;/p></description></item><item><title>Chapter 13.10: Entropy and Optimal Code Length II</title><link>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-10-sourcecoding2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-10-sourcecoding2/</guid><description>&lt;p>In this section, we continue our discussion on source coding and its relation to entropy.&lt;/p></description></item><item><title>Chapter 13.11: MI under Reparametrization: Deep Dive</title><link>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-11-mi-deepdive/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/13_information_theory/13-11-mi-deepdive/</guid><description>&lt;p>In this deep dive, we discuss the invariance of MI under certain reparametrizations.&lt;/p></description></item><item><title>Chapter 14.01: Curse of Dimensionality</title><link>https://slds-lmu.github.io/i2ml/chapters/14_cod/14-01-cod/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/14_cod/14-01-cod/</guid><description>&lt;p>In this section, we discuss why our geometric intuition fails in high-dimensional spaces and introduce the phenomenon of the curse of dimensionality.&lt;/p></description></item><item><title>Chapter 14.02: Curse of Dimensionality - Examples</title><link>https://slds-lmu.github.io/i2ml/chapters/14_cod/14-02-cod-examples/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/14_cod/14-02-cod-examples/</guid><description>&lt;p>In this section, we show examples of how \(k\)-NN and the linear model suffer from the the curse of dimensionality.&lt;/p></description></item><item><title>Chapter 15.01: Introduction to Regularization</title><link>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-01-regu-intro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-01-regu-intro/</guid><description>&lt;p>In this section, we revisit overfitting and introduce regularization as a remedy.&lt;/p></description></item><item><title>Chapter 15.02: Ridge Regression</title><link>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-02-l2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-02-l2/</guid><description>&lt;p>We introduce Ridge regression as a key approach to regularizing linear models.&lt;/p></description></item><item><title>Chapter 15.03: Lasso Regression</title><link>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-03-l1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-03-l1/</guid><description>&lt;p>We introduce Lasso regression as a key approach to regularizing linear models.&lt;/p></description></item><item><title>Chapter 15.04: Lasso vs Ridge Regression</title><link>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-04-l1vsl12/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-04-l1vsl12/</guid><description>&lt;p>This section provides a detailed comparison between Lasso and Ridge regression.&lt;/p></description></item><item><title>Chapter 15.05: Elastic Net and Regularization for GLMs</title><link>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-05-enetlogreg/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-05-enetlogreg/</guid><description>&lt;p>In this section, we introduce the elastic net as a combination of Ridge and Lasso regression and discuss regularization for logistic regression.&lt;/p></description></item><item><title>Chapter 15.06: Other Types of Regularization</title><link>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-06-other/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-06-other/</guid><description>&lt;p>In this section, we introduce other regularization approaches besides the important special cases \(L1\) and \(L2\).&lt;/p></description></item><item><title>Chapter 15.07: Non-Linear Models and Structural Risk Minimization</title><link>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-07-nonlin/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-07-nonlin/</guid><description>&lt;p>In this section, we demonstrate regularization in non-linear models like neural networks.&lt;/p></description></item><item><title>Chapter 15.08: Bayesian Priors</title><link>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-08-bayes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-08-bayes/</guid><description>&lt;p>In this section, we motivate regularization from a Bayesian perspective, showing how different penalty terms correspond to different Bayesian priors.&lt;/p></description></item><item><title>Chapter 15.09: Weight decay and L2</title><link>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-09-wd/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-09-wd/</guid><description>&lt;p>In this section, we show that L2 regularization with gradient descent is equivalent to weight decay and see how weight decay changes the optimization trajectory.&lt;/p></description></item><item><title>Chapter 15.10: Geometry of L2 Regularization</title><link>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-10-geom-l2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-10-geom-l2/</guid><description>&lt;p>In this section, we provide a geometric understanding of \(L2\) regularization, showing how parameters are shrunk according to the eigenvalues of the Hessian of empirical risk, and discuss its correspondence to weight decay.&lt;/p></description></item><item><title>Chapter 15.11: Geometry of L1 Regularization</title><link>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-11-geom-l1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-11-geom-l1/</guid><description>&lt;p>In this section, we provide a geometric understanding of \(L1\) regularization and show that it encourages sparsity in the parameter vector.&lt;/p></description></item><item><title>Chapter 15.12: Early Stopping</title><link>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-12-early-stopping/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-12-early-stopping/</guid><description>&lt;p>In this section, we introduce early stopping and show how it can act as a regularizer.&lt;/p></description></item><item><title>Chapter 15.13: Details on Ridge Regression: Deep Dive</title><link>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-13-ridge-deep/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-13-ridge-deep/</guid><description>&lt;p>In this section, we consider Ridge regression as row-augmentation and as minimizing risk under feature noise. We also discuss the bias-variance tradeoff.&lt;/p></description></item><item><title>Chapter 15.14: Soft-thresholding and L1 regularization: Deep Dive</title><link>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-14-lasso-deep/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/15_regularization/15-14-lasso-deep/</guid><description>&lt;p>In this section, we prove the previously stated proposition regarding soft-thresholding and L1 regularization.&lt;/p></description></item><item><title>Chapter 16.01: Linear Hard Margin SVM</title><link>https://slds-lmu.github.io/i2ml/chapters/16_linear_svm/16-01-hard-margin/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/16_linear_svm/16-01-hard-margin/</guid><description>&lt;p>Hard margin SVMs seek perfect data separation. We introduce the linear hard margin SVM problem as a quadratic optimization program.&lt;/p></description></item><item><title>Chapter 16.02: Hard Margin SVM Dual</title><link>https://slds-lmu.github.io/i2ml/chapters/16_linear_svm/16-02-hard-margin-dual/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/16_linear_svm/16-02-hard-margin-dual/</guid><description>&lt;p>In this section, we derive the dual variant of the linear hard-margin SVM problem, a computationally favorable formulation.&lt;/p></description></item><item><title>Chapter 16.03: Soft Margin SVM</title><link>https://slds-lmu.github.io/i2ml/chapters/16_linear_svm/16-03-soft-margin/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/16_linear_svm/16-03-soft-margin/</guid><description>&lt;p>Hard margin SVMs are often not applicable to practical questions because they fail when the data are not linearly separable. Moreover, for the sake of generalization, we will often accept some violations to keep the margin large enough for robust class separation. Therefore, we introduce the soft margin linear SVM.&lt;/p></description></item><item><title>Chapter 16.04: SVMs and Empirical Risk Minimization</title><link>https://slds-lmu.github.io/i2ml/chapters/16_linear_svm/16-04-erm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/16_linear_svm/16-04-erm/</guid><description>&lt;p>In this section, we show how the SVM problem can be understood as an instance of empirical risk minimization.&lt;/p></description></item><item><title>Chapter 16.05: SVM Training</title><link>https://slds-lmu.github.io/i2ml/chapters/16_linear_svm/16-05-optimization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/16_linear_svm/16-05-optimization/</guid><description>&lt;p>The linear SVM problem is challenging due to its non-differentiability. In this section, we present methods of optimization.&lt;/p></description></item><item><title>Chapter 17.01: Feature Generation for Nonlinear Separation</title><link>https://slds-lmu.github.io/i2ml/chapters/17_nonlinear_svm/17-01-featuregen/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/17_nonlinear_svm/17-01-featuregen/</guid><description>&lt;p>We show how nonlinear feature maps project the input data to transformed spaces, where they become linearly separable.&lt;/p></description></item><item><title>Chapter 17.02: The Kernel Trick</title><link>https://slds-lmu.github.io/i2ml/chapters/17_nonlinear_svm/17-02-kernel-trick/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/17_nonlinear_svm/17-02-kernel-trick/</guid><description>&lt;p>In this section, we show how nonlinear SVMs work their magic by introducing nonlinearity efficiently via the kernel trick.&lt;/p></description></item><item><title>Chapter 17.03: The Polynomial Kernel</title><link>https://slds-lmu.github.io/i2ml/chapters/17_nonlinear_svm/17-03-kernel-poly/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/17_nonlinear_svm/17-03-kernel-poly/</guid><description>&lt;p>In this section, we introduce the polynomial kernel in the context of SVMs and demonstrate how different polynomial degrees affect decision boundaries.&lt;/p></description></item><item><title>Chapter 17.04: Reproducing Kernel Hilbert Space and Representer Theorem</title><link>https://slds-lmu.github.io/i2ml/chapters/17_nonlinear_svm/17-04-rkhs-repr/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/17_nonlinear_svm/17-04-rkhs-repr/</guid><description>&lt;p>In this section, we introduce important theoretical background on nonlinear SVMs that essentially allows us to express them as a weighted sum of basis functions.&lt;/p></description></item><item><title>Chapter 17.05: The Gaussian RBF Kernel</title><link>https://slds-lmu.github.io/i2ml/chapters/17_nonlinear_svm/17-05-kernel-rbf/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/17_nonlinear_svm/17-05-kernel-rbf/</guid><description>&lt;p>In this section, we introduce the popular Gaussian RBF kernel and discuss its properties.&lt;/p></description></item><item><title>Chapter 17.06: SVM Model Selection</title><link>https://slds-lmu.github.io/i2ml/chapters/17_nonlinear_svm/17-06-model-sel/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/17_nonlinear_svm/17-06-model-sel/</guid><description>&lt;p>In this section, we discuss the importance of SVM hyperparameters for adequate solutions.&lt;/p></description></item><item><title>Chapter 18.01: Introduction to Boosting / AdaBoost</title><link>https://slds-lmu.github.io/i2ml/chapters/18_boosting/18-01-intro-adaboost/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/18_boosting/18-01-intro-adaboost/</guid><description>&lt;p>In this section, we introduce the pioneering AdaBoost algorithm.&lt;/p></description></item><item><title>Chapter 18.02: Boosting Concept</title><link>https://slds-lmu.github.io/i2ml/chapters/18_boosting/18-02-gradient-boosting-concept/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/18_boosting/18-02-gradient-boosting-concept/</guid><description>&lt;p>In this section, we discuss the general boosting principle: performing gradient descent in function space by repeatedly fitting new base learner components to the current pseudo-residuals.&lt;/p></description></item><item><title>Chapter 18.03: Boosting Illustration</title><link>https://slds-lmu.github.io/i2ml/chapters/18_boosting/18-03-gradient-boosting-illustration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/18_boosting/18-03-gradient-boosting-illustration/</guid><description>&lt;p>We show several illustrative regression examples to visualize the boosting
principle.&lt;/p></description></item><item><title>Chapter 18.04: Boosting Regularization</title><link>https://slds-lmu.github.io/i2ml/chapters/18_boosting/18-04-gradient-boosting-regularization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/18_boosting/18-04-gradient-boosting-regularization/</guid><description>&lt;p>Powerful boosting learners tend to overfit. We discuss the number of iterations, base learner complexity, and shrinkage as countermeasures.&lt;/p></description></item><item><title>Chapter 18.05: Boosting for Classification</title><link>https://slds-lmu.github.io/i2ml/chapters/18_boosting/18-05-gradient-boosting-classification/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/18_boosting/18-05-gradient-boosting-classification/</guid><description>&lt;p>We introduce boosting algorithms for both binary and multiclass classification with several examples.&lt;/p></description></item><item><title>Chapter 18.06: Gradient Boosting with Trees I</title><link>https://slds-lmu.github.io/i2ml/chapters/18_boosting/18-06-gradient-boosting-trees-1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/18_boosting/18-06-gradient-boosting-trees-1/</guid><description>&lt;p>We discuss trees as the most popular base learners in gradient boosting, with special emphasis on model structure and interaction depth.&lt;/p></description></item><item><title>Chapter 18.07: Gradient Boosting with Trees II</title><link>https://slds-lmu.github.io/i2ml/chapters/18_boosting/18-07-gradient-boosting-trees-2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/18_boosting/18-07-gradient-boosting-trees-2/</guid><description>&lt;p>We explain how terminal coefficients are found in a risk-minimal manner and briefly discuss tree-based boosting for multiclass problems.&lt;/p></description></item><item><title>Chapter 18.08: XGBoost</title><link>https://slds-lmu.github.io/i2ml/chapters/18_boosting/18-08-gradient-boosting-xgboost/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/18_boosting/18-08-gradient-boosting-xgboost/</guid><description>&lt;p>We introduce XGBoost, a highly efficient, tree-based boosting system with additional regularizers.&lt;/p></description></item><item><title>Chapter 18.09: Component Wise Boosting Basics 1</title><link>https://slds-lmu.github.io/i2ml/chapters/18_boosting/18-09-cwb-1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/18_boosting/18-09-cwb-1/</guid><description>&lt;p>We introduce the concept of CWB, common base learners and built-in feature selection.&lt;/p></description></item><item><title>Chapter 19.01: The Bayesian Linear Model</title><link>https://slds-lmu.github.io/i2ml/chapters/19_gaussian_processes/19-01-bayes-lm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/19_gaussian_processes/19-01-bayes-lm/</guid><description>&lt;p>We begin by reviewing the Bayesian formulation of a linear model and show that instead of point estimates for parameters and predictions, we obtain an entire posterior and predictive distribution.&lt;/p></description></item><item><title>Chapter 19.02: Gaussian Processes</title><link>https://slds-lmu.github.io/i2ml/chapters/19_gaussian_processes/19-02-basic/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/19_gaussian_processes/19-02-basic/</guid><description>&lt;p>In this section, we introduce the basic idea behind Gaussian processes. We move from weight to function space and build some intuition on distributions over functions, discuss GPs&amp;rsquo; marginalization property, derive GP priors, and interpret GPs as indexed families.&lt;/p></description></item><item><title>Chapter 19.03: Covariance Functions for GPs</title><link>https://slds-lmu.github.io/i2ml/chapters/19_gaussian_processes/19-03-covariance/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/19_gaussian_processes/19-03-covariance/</guid><description>&lt;p>In this section, we discuss the role of covariance functions in GPs and introduce the most common choices.&lt;/p></description></item><item><title>Chapter 19.04: Gaussian Process Prediction</title><link>https://slds-lmu.github.io/i2ml/chapters/19_gaussian_processes/19-04-prediction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/19_gaussian_processes/19-04-prediction/</guid><description>&lt;p>In this section, we show how to derive the posterior process and discuss further properties of GPs as well as noisy GPs.&lt;/p></description></item><item><title>Chapter 19.05: Gaussian Process Training</title><link>https://slds-lmu.github.io/i2ml/chapters/19_gaussian_processes/19-05-training/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/19_gaussian_processes/19-05-training/</guid><description>&lt;p>In this section, we show how Gaussian processes are actually trained using maximum likelihood estimation and exploiting the fact that we can learn covariance functions&amp;rsquo; hyperparameters on the fly.&lt;/p></description></item><item><title>Chapter 20.01: Introduction</title><link>https://slds-lmu.github.io/i2ml/chapters/20_imbalanced_learning/20-01-intro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/20_imbalanced_learning/20-01-intro/</guid><description>&lt;p>We define the phenomenon of imbalanced data sets and explain its consequences on accuarcy. Furthermore, we introduce some techniques for handling imbalanced data sets.&lt;/p></description></item><item><title>Chapter 20.02: Performance Measures</title><link>https://slds-lmu.github.io/i2ml/chapters/20_imbalanced_learning/20-02-perf-msr/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/20_imbalanced_learning/20-02-perf-msr/</guid><description>&lt;p>We introduce performance measures other than accuracy and explain their advantages over accuracy for imbalanced date. In addition we introduce extensions of these measures for multiclass settings.&lt;/p></description></item><item><title>Chapter 20.03: Cost-Sensitive Learning 1</title><link>https://slds-lmu.github.io/i2ml/chapters/20_imbalanced_learning/20-03-cs-1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/20_imbalanced_learning/20-03-cs-1/</guid><description>&lt;p>We introduce the concept of a Cost Matrix, the Minimum expected cost priciple and the optimal theoretical threshold.&lt;/p></description></item><item><title>Chapter 20.03: Cost-Sensitive Learning 1</title><link>https://slds-lmu.github.io/i2ml/chapters/20_imbalanced_learning/21-03-cs-1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/20_imbalanced_learning/21-03-cs-1/</guid><description>&lt;p>We introduce the concept of a Cost Matrix, the Minimum expected cost priciple and the optimal theoretical threshold.&lt;/p></description></item><item><title>Chapter 20.04: Cost-Sensitive Learning 2</title><link>https://slds-lmu.github.io/i2ml/chapters/20_imbalanced_learning/20-04-cs-2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/20_imbalanced_learning/20-04-cs-2/</guid><description>&lt;p>In this section we focus on empirical thresholding and model-agnostic Meta Costs.&lt;/p></description></item><item><title>Chapter 20.04: Cost-Sensitive Learning 2</title><link>https://slds-lmu.github.io/i2ml/chapters/20_imbalanced_learning/21-04-cs-2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/20_imbalanced_learning/21-04-cs-2/</guid><description>&lt;p>In this section we focus on empirical thresholding and model-agnostic Meta Costs.&lt;/p></description></item><item><title>Chapter 20.05: Cost-Sensitive Learning 3</title><link>https://slds-lmu.github.io/i2ml/chapters/20_imbalanced_learning/20-05-cs-3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/20_imbalanced_learning/20-05-cs-3/</guid><description>&lt;p>We explain the concepts of instance specific costs and cost-sensitive OVO.&lt;/p></description></item><item><title>Chapter 20.06: Cost Curves 1</title><link>https://slds-lmu.github.io/i2ml/chapters/20_imbalanced_learning/20-06-cc-1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/20_imbalanced_learning/20-06-cc-1/</guid><description>&lt;p>We introduce cost curves for misclassif error and explain the duality between ROC points and cost lines.&lt;/p></description></item><item><title>Chapter 20.07: Cost Curves 2</title><link>https://slds-lmu.github.io/i2ml/chapters/20_imbalanced_learning/20-07-cc-2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/20_imbalanced_learning/20-07-cc-2/</guid><description>&lt;p>We explain cost curves with cost matrices and comparing classifiers. In addition we do a wrap-up comparision to ROC.&lt;/p></description></item><item><title>Chapter 20.08: Sampling Methods 1</title><link>https://slds-lmu.github.io/i2ml/chapters/20_imbalanced_learning/20-08-smpl-1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/20_imbalanced_learning/20-08-smpl-1/</guid><description>&lt;p>We introduce the idea of sampling methods for dealing with imbalanced data. In addition, we explain certain undersampling techniques.&lt;/p></description></item><item><title>Chapter 20.09: Sampling Methods 2</title><link>https://slds-lmu.github.io/i2ml/chapters/20_imbalanced_learning/20-09-smpl-2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/20_imbalanced_learning/20-09-smpl-2/</guid><description>&lt;p>We introduce the state-of-art oversampling technique SMOTE.&lt;/p></description></item><item><title>Chapter 21.01: Introduction</title><link>https://slds-lmu.github.io/i2ml/chapters/21_multitarget_learning/21-01-intro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/21_multitarget_learning/21-01-intro/</guid><description>&lt;p>In this chapter we emphasize the practical relevance of multi-target prediction problems. In addition, we name some special cases of multi-target prediction and establish the differences between transductive and inductive learning problems.&lt;/p></description></item><item><title>Chapter 21.02: Loss functions</title><link>https://slds-lmu.github.io/i2ml/chapters/21_multitarget_learning/21-02-losses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/21_multitarget_learning/21-02-losses/</guid><description>&lt;p>In this chapter we introduce loss functions for multi-target prediction problems, explain the differences between instance-wise and decomposable losses and introduce the risk minimizer for both the hamming and 0/1 subset losses.&lt;/p></description></item><item><title>Chapter 21.03: Methods for Multi-target Prediction 1</title><link>https://slds-lmu.github.io/i2ml/chapters/21_multitarget_learning/21-03-methods-1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/21_multitarget_learning/21-03-methods-1/</guid><description>&lt;p>In this chapter we introduce the concepts of independent models for targets, mean regularization, stacking and weight sharing in DL.&lt;/p></description></item><item><title>Chapter 21.04: Methods for Multi-target Prediction 2</title><link>https://slds-lmu.github.io/i2ml/chapters/21_multitarget_learning/21-04-methods-2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/21_multitarget_learning/21-04-methods-2/</guid><description>&lt;p>In this chapter we introduce the Kronecker kernel ridge regression, graph relations in targets, probabilistic classifier chains and low-rank approximations.&lt;/p></description></item><item><title>Chapter 22.01: Introduction</title><link>https://slds-lmu.github.io/i2ml/chapters/22_online_learning/22-01-intro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/22_online_learning/22-01-intro/</guid><description>&lt;p>In this chapter we explain the differences between online and batch learning, the extended learning protocol in online learning and the strategies to measure performance in online learning.&lt;/p></description></item><item><title>Chapter 22.02: Simple Online Learning Algorithm</title><link>https://slds-lmu.github.io/i2ml/chapters/22_online_learning/22-02-simple/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/22_online_learning/22-02-simple/</guid><description>&lt;p>In this chapter we introduce the formalization of online learning algorithms and the FTL algorithm.&lt;/p></description></item><item><title>Chapter 22.03: Follow the Leader on OLO problems</title><link>https://slds-lmu.github.io/i2ml/chapters/22_online_learning/22-03-ftl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/22_online_learning/22-03-ftl/</guid><description>&lt;p>In this chapter we introduce OLO problems and explain why some FTL might fail on these problems.&lt;/p></description></item><item><title>Chapter 22.04: Follow the regularized Leader</title><link>https://slds-lmu.github.io/i2ml/chapters/22_online_learning/22-04-ftrl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/22_online_learning/22-04-ftrl/</guid><description>&lt;p>In this chapter we introduce FTLR as a stable alternative to FTL.&lt;/p></description></item><item><title>Chapter 22.05: Follow the Leader on OQO problems</title><link>https://slds-lmu.github.io/i2ml/chapters/22_online_learning/22-05-ftl-oqo/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/22_online_learning/22-05-ftl-oqo/</guid><description>&lt;p>In this chapter we prove that FTL works for online quadratic problems.&lt;/p></description></item><item><title>Chapter 22.06: Online Convex optimization 1</title><link>https://slds-lmu.github.io/i2ml/chapters/22_online_learning/22-06-oco-1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/22_online_learning/22-06-oco-1/</guid><description>&lt;p>In this chapter we introduce the class of online convex optimization problems and derive the online gradient descent as a suitable learning algorithm for such cases.&lt;/p></description></item><item><title>Chapter 22.07: Online Convex optimization 2</title><link>https://slds-lmu.github.io/i2ml/chapters/22_online_learning/22-07-oco-2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/22_online_learning/22-07-oco-2/</guid><description>&lt;p>In this chapter we explain the connection between OGD and FTRL via linearization of convex functions and how this implies regret bounds for OGD.&lt;/p></description></item><item><title>Chapter 30.01: Introduction</title><link>https://slds-lmu.github.io/i2ml/chapters/30_feature_selection/30-01-introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/30_feature_selection/30-01-introduction/</guid><description>&lt;p>We motivate feature selection and discuss the difference to feature extraction.&lt;/p></description></item><item><title>Chapter 30.02: Motivating Examples</title><link>https://slds-lmu.github.io/i2ml/chapters/30_feature_selection/30-02-motivating-examples/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/30_feature_selection/30-02-motivating-examples/</guid><description>&lt;p>In this section, we explain the practical importance of feature selection and show that models with
integrated selection do not always work.&lt;/p></description></item><item><title>Chapter 30.03: Filter Methods I</title><link>https://slds-lmu.github.io/i2ml/chapters/30_feature_selection/30-03-filters1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/30_feature_selection/30-03-filters1/</guid><description>&lt;p>We introduce how filter methods work and how they can be used for feature selection.&lt;/p></description></item><item><title>Chapter 30.04: Filter Methods II (Examples and Caveats)</title><link>https://slds-lmu.github.io/i2ml/chapters/30_feature_selection/30-04-filters2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/30_feature_selection/30-04-filters2/</guid><description>&lt;p>In this section, we discuss how filter methods can be misleading and show how they work in practical applications.&lt;/p></description></item><item><title>Chapter 30.05: Wrapper Methods</title><link>https://slds-lmu.github.io/i2ml/chapters/30_feature_selection/30-05-wrapper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/30_feature_selection/30-05-wrapper/</guid><description>&lt;p>This section explains wrapper methods and explains how they can aid feature selection.&lt;/p></description></item><item><title>Chapter 18.10: Component Wise Boosting Basics 2</title><link>https://slds-lmu.github.io/i2ml/chapters/18_boosting/18-10-cwb-2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/18_boosting/18-10-cwb-2/</guid><description>&lt;p>We explain the handling of categorical features and of the intercept and introduce a practical example.&lt;/p></description></item><item><title>Chapter 18.11: CWB and GLMs</title><link>https://slds-lmu.github.io/i2ml/chapters/18_boosting/18-11-cwb-glm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/18_boosting/18-11-cwb-glm/</guid><description>&lt;p>We explain the relationship between CWB and GLMs.&lt;/p></description></item><item><title>Chapter 18.12: Advanced CWB</title><link>https://slds-lmu.github.io/i2ml/chapters/18_boosting/18-12-adv-cwb/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/chapters/18_boosting/18-12-adv-cwb/</guid><description>&lt;p>We explain the details of nonlinear BLs and splines, decomposition for splines, fair base learner selection and feature importance and PDPs.&lt;/p></description></item><item><title>Cheat Sheets</title><link>https://slds-lmu.github.io/i2ml/appendix/01_cheat_sheets/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/appendix/01_cheat_sheets/</guid><description>&lt;ul>
&lt;li>I2ML :: BASICS&lt;/li>
&lt;/ul>


&lt;script src="https://cdn.jsdelivr.net/npm/pdfjs-dist@2.9.359/build/pdf.min.js" integrity="sha256-hEmjt7z3bB53X/awJyV81gmBLpVw2mj7EsvoJelZWow=" crossorigin="anonymous">&lt;/script>




 
 
 &lt;a href="https://github.com/slds-lmu/lecture_i2ml/raw/master/cheatsheets/cheatsheet_notation/cheatsheet_notation.pdf">
 &lt;button class="btn btn-primary" style="margin-bottom:3rem">
 Download &amp;raquo;cheatsheet_notation.pdf&amp;laquo;
 &lt;/button>
 &lt;/a>


&lt;ul>
&lt;li>I2ML :: EVALUATION &amp;amp; TUNING&lt;/li>
&lt;/ul>


&lt;script src="https://cdn.jsdelivr.net/npm/pdfjs-dist@2.9.359/build/pdf.min.js" integrity="sha256-hEmjt7z3bB53X/awJyV81gmBLpVw2mj7EsvoJelZWow=" crossorigin="anonymous">&lt;/script>




 
 
 &lt;a href="https://github.com/slds-lmu/lecture_i2ml/raw/master/cheatsheets/cheatsheet_eval_tuning/cheatsheet_eval_tuning.pdf">
 &lt;button class="btn btn-primary" style="margin-bottom:3rem">
 Download &amp;raquo;cheatsheet_eval_tuning.pdf&amp;laquo;
 &lt;/button>
 &lt;/a></description></item><item><title>Data sets</title><link>https://slds-lmu.github.io/i2ml/appendix/04_data/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/appendix/04_data/</guid><description>&lt;h2 id="data-sets-used-in-the-lecture">Data sets used in the lecture&lt;/h2>
&lt;ul>
&lt;li>Bikeshare&lt;/li>
&lt;/ul>


&lt;script src="https://cdn.jsdelivr.net/npm/pdfjs-dist@2.9.359/build/pdf.min.js" integrity="sha256-hEmjt7z3bB53X/awJyV81gmBLpVw2mj7EsvoJelZWow=" crossorigin="anonymous">&lt;/script>




 
 
 &lt;a href="https://github.com/slds-lmu/lecture_i2ml/raw/master/datasets-pdf/bikeshare.pdf">
 &lt;button class="btn btn-primary" style="margin-bottom:3rem">
 Download &amp;raquo;bikeshare.pdf&amp;laquo;
 &lt;/button>
 &lt;/a>


&lt;ul>
&lt;li>Boston Housing&lt;/li>
&lt;/ul>


&lt;script src="https://cdn.jsdelivr.net/npm/pdfjs-dist@2.9.359/build/pdf.min.js" integrity="sha256-hEmjt7z3bB53X/awJyV81gmBLpVw2mj7EsvoJelZWow=" crossorigin="anonymous">&lt;/script>




 
 
 &lt;a href="https://github.com/slds-lmu/lecture_i2ml/raw/master/datasets-pdf/boston.pdf">
 &lt;button class="btn btn-primary" style="margin-bottom:3rem">
 Download &amp;raquo;boston.pdf&amp;laquo;
 &lt;/button>
 &lt;/a>


&lt;p>We are aware of the ethical issues regarding this data set (&lt;a href="https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8">more information&lt;/a>) and will replace it in upcoming revisions of the material.&lt;/p>
&lt;ul>
&lt;li>Circle&lt;/li>
&lt;/ul>


&lt;script src="https://cdn.jsdelivr.net/npm/pdfjs-dist@2.9.359/build/pdf.min.js" integrity="sha256-hEmjt7z3bB53X/awJyV81gmBLpVw2mj7EsvoJelZWow=" crossorigin="anonymous">&lt;/script>




 
 
 &lt;a href="https://github.com/slds-lmu/lecture_i2ml/raw/master/datasets-pdf/circle.pdf">
 &lt;button class="btn btn-primary" style="margin-bottom:3rem">
 Download &amp;raquo;circle.pdf&amp;laquo;
 &lt;/button>
 &lt;/a>


&lt;ul>
&lt;li>German Credit&lt;/li>
&lt;/ul>


&lt;script src="https://cdn.jsdelivr.net/npm/pdfjs-dist@2.9.359/build/pdf.min.js" integrity="sha256-hEmjt7z3bB53X/awJyV81gmBLpVw2mj7EsvoJelZWow=" crossorigin="anonymous">&lt;/script>




 
 
 &lt;a href="https://github.com/slds-lmu/lecture_i2ml/raw/master/datasets-pdf/credit.pdf">
 &lt;button class="btn btn-primary" style="margin-bottom:3rem">
 Download &amp;raquo;credit.pdf&amp;laquo;
 &lt;/button>
 &lt;/a>


&lt;ul>
&lt;li>Glass&lt;/li>
&lt;/ul>


&lt;script src="https://cdn.jsdelivr.net/npm/pdfjs-dist@2.9.359/build/pdf.min.js" integrity="sha256-hEmjt7z3bB53X/awJyV81gmBLpVw2mj7EsvoJelZWow=" crossorigin="anonymous">&lt;/script>




 
 
 &lt;a href="https://github.com/slds-lmu/lecture_i2ml/raw/master/datasets-pdf/glass.pdf">
 &lt;button class="btn btn-primary" style="margin-bottom:3rem">
 Download &amp;raquo;glass.pdf&amp;laquo;
 &lt;/button>
 &lt;/a>


&lt;ul>
&lt;li>Ionosphere&lt;/li>
&lt;/ul>


&lt;script src="https://cdn.jsdelivr.net/npm/pdfjs-dist@2.9.359/build/pdf.min.js" integrity="sha256-hEmjt7z3bB53X/awJyV81gmBLpVw2mj7EsvoJelZWow=" crossorigin="anonymous">&lt;/script>




 
 
 &lt;a href="https://github.com/slds-lmu/lecture_i2ml/raw/master/datasets-pdf/ionosphere.pdf">
 &lt;button class="btn btn-primary" style="margin-bottom:3rem">
 Download &amp;raquo;ionosphere.pdf&amp;laquo;
 &lt;/button>
 &lt;/a>


&lt;ul>
&lt;li>Iris&lt;/li>
&lt;/ul>


&lt;script src="https://cdn.jsdelivr.net/npm/pdfjs-dist@2.9.359/build/pdf.min.js" integrity="sha256-hEmjt7z3bB53X/awJyV81gmBLpVw2mj7EsvoJelZWow=" crossorigin="anonymous">&lt;/script>




 
 
 &lt;a href="https://github.com/slds-lmu/lecture_i2ml/raw/master/datasets-pdf/iris.pdf">
 &lt;button class="btn btn-primary" style="margin-bottom:3rem">
 Download &amp;raquo;iris.pdf&amp;laquo;
 &lt;/button>
 &lt;/a>


&lt;p>We are aware of the ethical issues regarding this data set (&lt;a href="https://armchairecology.blog/iris-dataset/">more information&lt;/a>) and will replace it in upcoming revisions of the material.&lt;/p></description></item><item><title>Errata</title><link>https://slds-lmu.github.io/i2ml/appendix/02_errata/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/appendix/02_errata/</guid><description>&lt;h2 id="errata-in-the-slides-shown-in-the-videos">Errata in the slides shown in the videos&lt;/h2>
&lt;ul>
&lt;li>Chapter 1.4 (Models &amp;amp; Parameters) - slide 5/10: d-dimensional vector, not p-dimensional&lt;/li>
&lt;li>Chapter 2.1 (Regression losses): Slide 1/5 sign in bullet point 4&lt;/li>
&lt;li>Chapter 2.2 (Deep Dive OLS): Slide 2/2 last lines in left column&lt;/li>
&lt;li>Chapter 3.6 (Naive Bayes): Slide 3/6: Missing exponents in figure&lt;/li>
&lt;li>Chapter 4.3 (Simple Measures for Classification) - slide 6/9: Error in cost matrix&lt;/li>
&lt;li>Chapter 4.4 (Perfomance Evaluation: Test Error) - slide 8/13: The variance of MSE decreases with test set size, not the mean of MSE&lt;/li>
&lt;li>Chapter 4.7 (Classification measures): Slide 6/9 cost computation&lt;/li>
&lt;li>Chapter 6.2 (CART: Growing a Tree) - slide 5/8: Wrong plot is displayed in video (axis wrong, points missing)&lt;/li>
&lt;li>Chapter 11.6 (0-1 Loss): Slides 2/5 and 4/5 Errors in notation of conditional probability inside of expectation&lt;/li>
&lt;li>Chapter 11.7 (Bernoulli Loss): Slides 9/10 and 10/10 Errors in Bernoulli Loss and Entropy Splitting Criterion&lt;/li>
&lt;li>Chapter 11.12 (MLE2): Slide 2/5 wrong negative sign in NLL equation&lt;/li>
&lt;li>Chapter 12.2 (Softmax): Slide 2/9 [0,1]^g instead of R^g&lt;/li>
&lt;li>Chapter 13.1 (Entropy I): Slide 4,6,8/10 changed entropy calculation from nats to bits&lt;/li>
&lt;li>Chapter 13.2 (Entropy II): Slide 1/7 corrected plot for entropy of Bernoulli distribution&lt;/li>
&lt;li>Chapter 13.5 (CE-KLD): Slide 6/7 typo in formula (1)&lt;/li>
&lt;li>Chapter 13.6: Slide 2/7 typos in formula and bullet point 3&lt;/li>
&lt;li>Chapter 13.7: Slide 4/14 switched x and y in the proposition regarding zero conditional entropy&lt;/li>
&lt;li>Chapter 13.7: Slide 14/14 added missing 0.5 factor in the entropy of the multivariate Gaussian&lt;/li>
&lt;li>Chapter 13.7: Slide 14/14 added parentheses to make log less ambiguous&lt;/li>
&lt;li>Chapter 15.2 (Ridge Regression): Slide 4/10 clarified meaning of green dot in plot in comment&lt;/li>
&lt;li>Chapter 15.8 (Bayesian Priors): Slide 5/5 renew plot and add comment on ybar&lt;/li>
&lt;li>Chapter 17.1: Slide 6/11 Typo in f&lt;/li>
&lt;li>Chapter 17.3: Slide 2/6 Show how to get phi(x) explicitly, and the last component of phi(x) should be b instead of sqrt(b).&lt;/li>
&lt;li>Chapter 17.3: Slide 2/7 Assume gamma = 1 to avoid conflict with geometric distances&lt;/li>
&lt;li>Chapter 18.1: Slide 5/7: Typo in Monomials; it should be (d+p-1 over d) and ( (d+p) over d) -1&lt;/li>
&lt;li>Chapter 19.2: Slide 1/15: R_emp formula contains x_{i}, not x.&lt;/li>
&lt;li>Chapter 19.5: Slide 3/11: bullet point 1 added &amp;ldquo;if&amp;rdquo;, bullet point 2 changed &amp;ldquo;decrease&amp;rdquo; to &amp;ldquo;increase&amp;rdquo;&lt;/li>
&lt;li>Chapter 20.1: Slide 6/12: fix error in step 6 and clarify that b-hat is a hard label classifier&lt;/li>
&lt;li>Chapter 20.1: Slides 7 and 8/12: improved plot and text&lt;/li>
&lt;li>Chapter 20.2: Slide 14/15: error in step 1 of the algorithm&lt;/li>
&lt;li>Chapter 21.4: Slide 2/5: increased number of boosting iterations from 1 to 300 in plot&lt;/li>
&lt;li>Chapter 21.4: Slide 5/5: add to for loops over i=1,..,n samples and initialize empty data set&lt;/li>
&lt;li>Chapter 13.1: Slide 10/10: fixed incorrect derivative&lt;/li>
&lt;li>Chapter 18.08: Slide 6/8: the slide set is correct, i.e., the given percentage is used to &amp;ldquo;dropout&amp;rdquo; / ignore the learners. (This is currently incorrectly stated in the video)&lt;/li>
&lt;/ul></description></item><item><title>Important Learners in ML</title><link>https://slds-lmu.github.io/i2ml/appendix/00_learner_slides/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/appendix/00_learner_slides/</guid><description>&lt;h2 id="look-up-slides-for-important-ml-learners">Look-up slides for important ML learners&lt;/h2>


&lt;script src="https://cdn.jsdelivr.net/npm/pdfjs-dist@2.9.359/build/pdf.min.js" integrity="sha256-hEmjt7z3bB53X/awJyV81gmBLpVw2mj7EsvoJelZWow=" crossorigin="anonymous">&lt;/script>




 
 
 &lt;a href="https://github.com/slds-lmu/lecture_i2ml/raw/master/learners-overview/slides-learners.pdf">
 &lt;button class="btn btn-primary" style="margin-bottom:3rem">
 Download &amp;raquo;slides-learners.pdf&amp;laquo;
 &lt;/button>
 &lt;/a></description></item><item><title>Related Courses</title><link>https://slds-lmu.github.io/i2ml/appendix/03_related/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://slds-lmu.github.io/i2ml/appendix/03_related/</guid><description>&lt;h2 id="other-ml-courses">Other ML courses&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://slds-lmu.github.io/dl4nlp/">Deep Learning for NLP (DL4NLP)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://slds-lmu.github.io/i2dl/">Introduction to Deep Learning (I2DL)&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>